<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Knowledge snippet | Edo</title>
    <link>https://edoardocostantini.github.io/category/knowledge-snippet/</link>
      <atom:link href="https://edoardocostantini.github.io/category/knowledge-snippet/index.xml" rel="self" type="application/rss+xml" />
    <description>Knowledge snippet</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Tue, 06 Sep 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://edoardocostantini.github.io/media/icon_hubc644763d821b4643633e77d54abca59_619947_512x512_fill_lanczos_center_3.png</url>
      <title>Knowledge snippet</title>
      <link>https://edoardocostantini.github.io/category/knowledge-snippet/</link>
    </image>
    
    <item>
      <title>Understanding quantiles</title>
      <link>https://edoardocostantini.github.io/post/quantiles/</link>
      <pubDate>Tue, 06 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://edoardocostantini.github.io/post/quantiles/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;

&lt;/div&gt;

&lt;div id=&#34;quantiles-percentiles-and-quartiles&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Quantiles, percentiles, and quartiles&lt;/h2&gt;
&lt;p&gt;Descriptive statistics are a fundamental tools for a data analysis.
Their purpose is to describe and summarize data.
Quantitative variables such as weight, age, and income can be described by distributions (e.g., normal distribution) with measures of &lt;strong&gt;center&lt;/strong&gt; (the typical value of a variable) and &lt;strong&gt;variability&lt;/strong&gt; (spread around the center).
The mean is a measure of center.
The standard deviation is a measure of variability.&lt;/p&gt;
&lt;p&gt;There are special descriptive statistics that help us describe simultaneously center and spread of a variableâ€™s distribution.
These measures are often known as measures of &lt;strong&gt;positions&lt;/strong&gt;.
In general, they tell us the point of the distribution of a variable at which a given percentage of the data falls below or above that point.
The minimum and maximum values of a variable are measures of positions defining the point at which no data, or all data, fall below it, respectively.
The median is a measure of position that defines the point at which half of the data falls below it (and above it).
The median is a special case of the measure of position called &lt;strong&gt;percentile&lt;/strong&gt;.
The p-th percentile is the point such that p% of the observations fall below or at that point and (100 - p)% fall above it.
The 50-th percentile is the median.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Quartiles&lt;/strong&gt; are commonly used percentiles.
The first quartile is the 25-th percentile, the value of the variable leaving to its left 25% of the distribution.
The third quartile is the 75-th percentile, the value of the variable leaving to its left 75% of the distribution.
You can think of quartiles as diving the probability distribution in 4 intervals with equal probabilities (e.g., area under the probability density function).
Similarly, you can think of percentiles as diving the probability distribution in 100 intervals with equal probabilities.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://edoardocostantini.github.io/post/quantiles/index_files/figure-html/quartile-pic-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We use the word &lt;strong&gt;quantile&lt;/strong&gt; to describe the general measure of position that divides the probability distribution in intervals with equal probabilities.
Percentiles divide the probability distribution of a variable in 100 intervals.
Deciles divide it in 10 intervals, and quartiles in four.
As such, percentiles, deciles, quartiles, and the median are all special cases of quantiles.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tldr-just-give-me-the-code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;TL;DR, just give me the code!&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Set a seed

set.seed(20220906)

# Sample from a normal distribution

age &amp;lt;- rnorm(1e5, mean = 27, sd = 2)

# Define the 1st and 2nd quartile

quartiles &amp;lt;- quantile(age, probs = c(.25, .75))

# Plot density distribution

plot(density(age),
     main = &amp;quot;Quartiles for the probability distribution of age&amp;quot;,
     xlab = NA)

# Costum x ticks
axis(side = 1, at = c(27, round(quartiles, 1)), labels = TRUE)

# Add points for quantiles
points(c(quartiles, median(age)),
        y = rep(0, length(quartiles)+1))
points(c(quartiles, median(age)),
       y = dnorm(c(quartiles, median(age)), mean = mean(age), sd = sd(age)))

# Add segments to devide plot
segments(x0 = quartiles[1], y0 = 0,
         x1 = quartiles[1], y1 = dnorm(quartiles[1],
                                       mean = mean(age), sd = sd(age)))
segments(x0 = median(age), y0 = 0,
         x1 = median(age), y1 = max(dnorm(age,
                                       mean = mean(age), sd = sd(age))))
segments(x0 = quartiles[2], y0 = 0,
         x1 = quartiles[2], y1 = dnorm(quartiles[2],
                                       mean = mean(age), sd = sd(age)))

# Add quartile labels
text(x = quartiles[1],
     y = -.005,
     &amp;quot;1st quartile&amp;quot;)
text(x = quartiles[2],
     y = -.005,
     &amp;quot;3rd quartile&amp;quot;)

# Add percentage under the curve labels
text(x = c(24, 30, 26.3, 27.7),
     y = c(.03, .03, .06, .06),
     &amp;quot;25 %&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Understanding boxplots</title>
      <link>https://edoardocostantini.github.io/post/boxplots/</link>
      <pubDate>Mon, 05 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://edoardocostantini.github.io/post/boxplots/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;

&lt;/div&gt;

&lt;div id=&#34;reading-a-boxplot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Reading a boxplot&lt;/h2&gt;
&lt;p&gt;Boxplots are descriptive tools to visualize the distribution of variables with a focus on their measures of spread and center.
A boxplots report in the same figure the median, the 1st and 3rd quartiles, and indicate possible outliers.&lt;/p&gt;
&lt;p&gt;Imagine wanting to plot the distribution of age in a court of students enrolled in a master program at a university.
The age of the students is likely to be normally distributed around a mean of 26.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Set up ----------------------------------------------------------------------

# Set seed
set.seed(20220906)

# Generate some age variable for a university master programme
age &amp;lt;- round(rnorm(1e3, mean = 26, sd = 2), 0)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, we can create the boxplot of this age variable in R by using the &lt;code&gt;boxplot()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Look at the boxplot ---------------------------------------------------------
boxplot(age)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://edoardocostantini.github.io/post/boxplots/index_files/figure-html/boxplot-explained-1.png&#34; width=&#34;720&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The variable age is centered around 26 and 50% of the distribution is located between 25 (1st quartile) and 27 (3rd quartile).
There are 6 values that represent possible outliers (the circles outside the whiskers).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;play-around-with-boxplots&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Play around with boxplots&lt;/h2&gt;
&lt;p&gt;You can compute the statistics used to draw the boxplot explicitly by following this code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Compute boxplot statistics manually ------------------------------------------

# Compute the median
med &amp;lt;- median(age)

# Compute 1st and 3rd quartiles
qnt &amp;lt;- quantile(age, probs = c(.25, .75))

# Compute interquartile range
IQR &amp;lt;- diff(qnt)[[1]]

# Compute fences/whisker bounds
C &amp;lt;- 1.5 # range multiplier
fences &amp;lt;- c(lwr = qnt[[1]] - C * IQR, upr = qnt[[2]] + C * IQR)

# Put together the boxplot stats
bxstats &amp;lt;- sort(c(med = med, qnt, f = fences))

# Compute boxplot statistics with R function
bxstats_auto &amp;lt;- boxplot.stats(age, coef = C)$stats

# Compare results obtain manually and with the R function
data.frame(manual = bxstats, R.function = bxstats_auto)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       manual R.function
## f.lwr     22         22
## 25%       25         25
## med       26         26
## 75%       27         27
## f.upr     30         30&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can visualize the impact of different choices for the range multiplier &lt;code&gt;C&lt;/code&gt;.
In the following pictures, you can see that a larger &lt;code&gt;C&lt;/code&gt; is less restrictive in which values are considered outliers.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://edoardocostantini.github.io/post/boxplots/index_files/figure-html/boxplot-c-1.png&#34; width=&#34;1344&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tldr-just-give-me-the-code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;TL;DR, just give me the code!&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Set up ----------------------------------------------------------------------

# Set seed
set.seed(20220906)

# Generate some age variable for a university master programme
age &amp;lt;- round(rnorm(1e3, mean = 26, sd = 2), 0)

# Look at the boxplot ---------------------------------------------------------
boxplot(age)

# Boxplot with explanation
C &amp;lt;- 1.5 # range multiplier
boxplot(age, range = C)

# Add arrows pointings to statistics
arrows(x0 = .69, y0 = boxplot.stats(age, coef = C)$stats,
       x1 = c(.875, rep(.765, 3), .875), y1 = boxplot.stats(age, coef = C)$stats,
       length = 0.1)

# Add labels of statistics
text(x = rep(.66, 5),
     y = boxplot.stats(age, coef = C)$stats,
     labels = c(&amp;quot;lower whisker&amp;quot;,
                &amp;quot;1st quartile&amp;quot;,
                &amp;quot;median&amp;quot;,
                &amp;quot;3rd quartile&amp;quot;,
                &amp;quot;upper whisker&amp;quot;),
     adj = 1)

# Add y axis labels
axis(side = 2, at = boxplot.stats(age, coef = C)$stats[c(1, 3, 4)], labels = TRUE)

# Compute boxplot statistics manually ------------------------------------------

# Compute the median
med &amp;lt;- median(age)

# Compute 1st and 3rd quartiles
qnt &amp;lt;- quantile(age, probs = c(.25, .75))

# Compute interquartile range
IQR &amp;lt;- diff(qnt)[[1]]

# Compute fences/whisker bounds
C &amp;lt;- 1.5 # range multiplier
fences &amp;lt;- c(lwr = qnt[[1]] - C * IQR, upr = qnt[[2]] + C * IQR)

# Put together the boxplot stats
bxstats &amp;lt;- sort(c(med = med, qnt, f = fences))

# Compute boxplot statistics with R function
bxstats_auto &amp;lt;- boxplot.stats(age, coef = C)$stats

# Compare results obtain manually and with the R function
data.frame(manual = bxstats, R.function = bxstats_auto)

# Visualize the effect of different C -----------------------------------------

# Allow two plots one next to the other
par(mfrow = c(1, 2))

# Plot C = 1.5 and 3
lapply(c(1.5, 3.0), FUN = function (x){
  C &amp;lt;- x
  boxplot(age, range = C, main = paste0(&amp;quot;C = &amp;quot;, C))

  # Add arrows pointings to statistics
  arrows(x0 = .69, y0 = boxplot.stats(age, coef = C)$stats,
         x1 = c(.875, rep(.765, 3), .875), y1 = boxplot.stats(age, coef = C)$stats,
         length = 0.1)
  # Add labels of statistics
  text(x = rep(.66, 5),
       y = boxplot.stats(age, coef = C)$stats,
       labels = c(paste(ifelse(C == 1.5, &amp;quot;inner&amp;quot;, &amp;quot;outer&amp;quot;), &amp;quot;fence \n lower bound&amp;quot;),
                  &amp;quot;1st quartile&amp;quot;,
                  &amp;quot;median&amp;quot;,
                  &amp;quot;3rd quartile&amp;quot;,
                  paste(ifelse(C == 1.5, &amp;quot;inner&amp;quot;, &amp;quot;outer&amp;quot;), &amp;quot;fence \n upper bound&amp;quot;)),
       adj = 1)
})&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Understanding the residual standard error</title>
      <link>https://edoardocostantini.github.io/post/residual-standard-error/</link>
      <pubDate>Wed, 22 Jun 2022 00:00:00 +0000</pubDate>
      <guid>https://edoardocostantini.github.io/post/residual-standard-error/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;

&lt;/div&gt;

&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The residual standard error is a measure of fit for linear regression models.
Conceptually, it can be thought of as the variability of the prediction error for a linear model.
It is usually calculated as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
SE_{resid} = \sqrt{ \frac{ \sum^{n}_{i = 1}(y_i - \hat{y}_i)^2 }{df_{resid}} }
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is the sample size&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; is the number of parameters to estimate in the model&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(-1\)&lt;/span&gt; is the degree of freedom lost to estimate the intercept&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat{y}_i\)&lt;/span&gt; is the fitted &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; value for the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th individual&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(df_{resid}\)&lt;/span&gt; is the degrees of freedom of the residuals (&lt;span class=&#34;math inline&#34;&gt;\(n - k - 1\)&lt;/span&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The smaller the residual standard error, the better the model fits the data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;learn-by-coding&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Learn by coding&lt;/h2&gt;
&lt;p&gt;We can compute the residual standard error manually after estimating a linear model in R.
To get a better grasp of the residual standard error, letâ€™s start by regressing the miles per gallon (mpg) on the number of cylinders (cyl), horsepower (hp), and weight (wt) of cars from the standard &lt;code&gt;mtcars&lt;/code&gt; R dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Fit a linear model -----------------------------------------------------------

  lm_fit &amp;lt;- lm(mpg ~ cyl + hp + wt, data = mtcars)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can compute the residual standard error following the formula described above:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Compute the residual standard error manually ---------------------------------

  # Define elements of the formula
  n &amp;lt;- nrow(mtcars) # sample size
  k &amp;lt;- 3            # number of parameters (regression coefficients)
  yhat &amp;lt;- fitted(lm_fit) # fitted y values
  y &amp;lt;- mtcars$mpg

  # Compute rse
  rse &amp;lt;- sqrt(sum((y - yhat)^2) / (n - k - 1))

  # Print rse
  rse&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.511548&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also extract it directly from any &lt;code&gt;lm&lt;/code&gt; object:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# residual standard error from lm output ---------------------------------------

  # Use the sigma function to extract it from an lm object
  sigma(lm_fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.511548&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  # Compare with the manual computation
  sigma(lm_fit) - rse&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;tldr-just-give-me-the-code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;TL;DR, just give me the code!&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Fit a linear model -----------------------------------------------------------

  lm_fit &amp;lt;- lm(mpg ~ cyl + hp + wt, data = mtcars)

# Compute the residual standard error manually ---------------------------------

  # Define elements of the formula
  n &amp;lt;- nrow(mtcars) # sample size
  k &amp;lt;- 3            # number of parameters (regression coefficients)
  yhat &amp;lt;- fitted(lm_fit) # fitted y values
  y &amp;lt;- mtcars$mpg

  # Compute rse
  rse &amp;lt;- sqrt(sum((y - yhat)^2) / (n - k - 1))

  # Print rse
  rse

# residual standard error from lm output ---------------------------------------

  # Use the sigma function to extract it from an lm object
  sigma(lm_fit)

  # Compare with the manual computation
  sigma(lm_fit) - rse&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;other-resources&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Other resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.statology.org/how-to-interpret-residual-standard-error/&#34;&gt;Statology: How to Interpret Residual Standard Error&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.statology.org/residual-standard-error-r/&#34;&gt;Statology: How to Calculate Residual Standard Error in R&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
