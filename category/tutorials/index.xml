<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tutorials | Edo</title>
    <link>https://edoardocostantini.github.io/category/tutorials/</link>
      <atom:link href="https://edoardocostantini.github.io/category/tutorials/index.xml" rel="self" type="application/rss+xml" />
    <description>Tutorials</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Fri, 22 Apr 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://edoardocostantini.github.io/media/icon_hud9fa8ac6cd4a8ee42e0d32197c4c8f6f_199053_512x512_fill_lanczos_center_3.png</url>
      <title>Tutorials</title>
      <link>https://edoardocostantini.github.io/category/tutorials/</link>
    </image>
    
    <item>
      <title>Cross-entropy as a measure of predictive accuracy</title>
      <link>https://edoardocostantini.github.io/post/cross-entropy/</link>
      <pubDate>Fri, 22 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://edoardocostantini.github.io/post/cross-entropy/</guid>
      <description>
&lt;script src=&#34;https://edoardocostantini.github.io/post/cross-entropy/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1&lt;/span&gt; Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#learn-by-coding&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2&lt;/span&gt; Learn by coding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tldr-just-give-me-the-code&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3&lt;/span&gt; TL;DR, just give me the code!&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#other-resources&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4&lt;/span&gt; Other resources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34; number=&#34;1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Introduction&lt;/h1&gt;
&lt;p&gt;Cross-entropy quantifies the difference between two probability distributions.
As such, it comes in handy as a &lt;a href=&#34;https://en.wikipedia.org/wiki/Loss_function&#34;&gt;loss function&lt;/a&gt; in multi-class classification tasks (e.g., multinomial logistic regression).
Cross-entropy also provides an elegant solution for determining the difference between actual and predicted categorical data point values.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;learn-by-coding&#34; class=&#34;section level1&#34; number=&#34;2&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Learn by coding&lt;/h1&gt;
&lt;p&gt;Bla bla&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Prepare environment ----------------------------------------------------------

# Packages
library(nnet)

# Data
dat_bin &amp;lt;- ToothGrowth
dat_cat &amp;lt;- iris&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First, binary cross entropy&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Binary cross entropy ---------------------------------------------------------

# Fit model
glm_log &amp;lt;- glm(supp ~ .,
               family = binomial(link = &amp;#39;logit&amp;#39;),
               data = ToothGrowth)

# Predictions
preds_log &amp;lt;- predict(glm_log, type = &amp;quot;response&amp;quot;)

# Compute entropy
p &amp;lt;- as.numeric(ToothGrowth$supp)-1 #
phat &amp;lt;- preds_log

x &amp;lt;- 0
for (i in 1:length(p)){
  x &amp;lt;- x + (p[i] * log(phat[i]))
}

- x&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        1 
## 17.99278&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Multi-categorical cross entropy ----------------------------------------------

# Fit model
glm_mln &amp;lt;- multinom(Species ~ ., data = iris)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # weights:  18 (10 variable)
## initial  value 164.791843 
## iter  10 value 16.177348
## iter  20 value 7.111438
## iter  30 value 6.182999
## iter  40 value 5.984028
## iter  50 value 5.961278
## iter  60 value 5.954900
## iter  70 value 5.951851
## iter  80 value 5.950343
## iter  90 value 5.949904
## iter 100 value 5.949867
## final  value 5.949867 
## stopped after 100 iterations&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Predictions
preds_mln &amp;lt;- predict(glm_mln, type = &amp;quot;probs&amp;quot;)

# Compute entropy
p &amp;lt;- FactoMineR::tab.disjonctif(iris$Species)
phat &amp;lt;- preds_mln

x &amp;lt;- 0
for (i in 1:nrow(p)){
  for (j in 1:ncol(p)){
    x &amp;lt;- x + (p[i, ] * log(phat[i, ]))
  }
}

- sum(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 17.8496&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Fast way to compute both -----------------------------------------------------

ce &amp;lt;- - sum(
  diag(
    p %*% t(log(phat))
  )
)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;tldr-just-give-me-the-code&#34; class=&#34;section level1&#34; number=&#34;3&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; TL;DR, just give me the code!&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Prepare environment ----------------------------------------------------------

# Packages
library(nnet)

# Data
dat_bin &amp;lt;- ToothGrowth
dat_cat &amp;lt;- iris

# Binary cross entropy ---------------------------------------------------------

# Fit model
glm_log &amp;lt;- glm(supp ~ .,
               family = binomial(link = &amp;#39;logit&amp;#39;),
               data = ToothGrowth)

# Predictions
preds_log &amp;lt;- predict(glm_log, type = &amp;quot;response&amp;quot;)

# Compute entropy
p &amp;lt;- as.numeric(ToothGrowth$supp)-1 #
phat &amp;lt;- preds_log

x &amp;lt;- 0
for (i in 1:length(p)){
  x &amp;lt;- x + (p[i] * log(phat[i]))
}

- x

# Multi-categorical cross entropy ----------------------------------------------

# Fit model
glm_mln &amp;lt;- multinom(Species ~ ., data = iris)

# Predictions
preds_mln &amp;lt;- predict(glm_mln, type = &amp;quot;probs&amp;quot;)

# Compute entropy
p &amp;lt;- FactoMineR::tab.disjonctif(iris$Species)
phat &amp;lt;- preds_mln

x &amp;lt;- 0
for (i in 1:nrow(p)){
  for (j in 1:ncol(p)){
    x &amp;lt;- x + (p[i, ] * log(phat[i, ]))
  }
}

- sum(x)

# Fast way to compute both -----------------------------------------------------

ce &amp;lt;- - sum(
  diag(
    p %*% t(log(phat))
  )
)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;other-resources&#34; class=&#34;section level1&#34; number=&#34;4&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Other resources&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://rpubs.com/juanhklopper/cross_entropy&#34;&gt;Cross-entropy in RPubs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://machinelearningmastery.com/cross-entropy-for-machine-learning/&#34;&gt;A Gentle Introduction to Cross-Entropy for Machine Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://gombru.github.io/2018/05/23/cross_entropy_loss/&#34;&gt;Understanding Categorical Cross-Entropy Loss, Binary Cross-Entropy Loss, Softmax Loss, Logistic Loss, Focal Loss and all those confusing names&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Estimating the weighted covariance matrix in R</title>
      <link>https://edoardocostantini.github.io/post/covmatwt/</link>
      <pubDate>Mon, 14 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://edoardocostantini.github.io/post/covmatwt/</guid>
      <description>
&lt;script src=&#34;https://edoardocostantini.github.io/post/covmatwt/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1&lt;/span&gt; Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#learn-by-coding&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2&lt;/span&gt; Learn by coding&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#example&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.1&lt;/span&gt; Example&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#computing-the-weighted-covariance-matrix-manually&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.2&lt;/span&gt; Computing the weighted covariance matrix manually&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#exploring-the-statscov.wt-function-code&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.2.1&lt;/span&gt; Exploring the &lt;code&gt;stats::cov.wt()&lt;/code&gt; function code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#reproducing-the-internal-steps&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.2.2&lt;/span&gt; Reproducing the internal steps&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mathematical-formula-and-alternative-r-computations&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.3&lt;/span&gt; Mathematical formula and alternative R computations&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#unbiased-weighted-covariance-matrix&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.3.1&lt;/span&gt; Unbiased weighted covariance matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#maximum-likelihood-weighted-covariance-matrix&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.3.2&lt;/span&gt; Maximum Likelihood weighted covariance matrix&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#relationship-with-the-matrix-of-sufficient-statistics&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.4&lt;/span&gt; Relationship with the matrix of sufficient statistics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tldr-just-give-me-the-code&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3&lt;/span&gt; TL;DR, just give me the code!&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34; number=&#34;1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Introduction&lt;/h1&gt;
&lt;p&gt;In a sample made of groups of different sizes, descriptive statistics like the mean and the covariance between variables can be computed by assigning proper weights to account for the difference in group sizes.
Wights are generally normalized (i.e., &lt;span class=&#34;math inline&#34;&gt;\(\sum_{i = 1}^{n} w_i = 1\)&lt;/span&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;learn-by-coding&#34; class=&#34;section level1&#34; number=&#34;2&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Learn by coding&lt;/h1&gt;
&lt;div id=&#34;example&#34; class=&#34;section level2&#34; number=&#34;2.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.1&lt;/span&gt; Example&lt;/h2&gt;
&lt;p&gt;Now, let’s consider a very simple example.
Say that you have a dataset with two variables and that you have a vector of weights defining how important each observation should be.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Initial simple example -------------------------------------------------------

  # Get the dataset used in the example of stats::cov.wt()
  xy &amp;lt;- cbind(x = 1:10, y = c(1:3, 8:5, 8:10))

  # Define non-negative weights (as in example of stats::cov.wt())
  wi &amp;lt;- c(0,0,0,1,1,1,1,1,0,0)

  # Get the weighted estimate with the default methods
  covwt_stats &amp;lt;- stats::cov.wt(xy, wt = wi) # i.e. method = &amp;quot;unbiased&amp;quot;

  # Compare unweighted and weighted means
  data.frame(uw = colMeans(xy),
             select = colMeans(xy[wi == 1, ]),
             wg = covwt_stats$center)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    uw select  wg
## x 5.5    6.0 6.0
## y 5.9    6.8 6.8&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  # Compare unweighted and weighted covariance matrix
  data.frame(uw = c(cov(xy)),
             select = c(cov(xy[wi == 1, ])),
             wg = c(covwt_stats$cov),
             row.names = c(sapply(colnames(cov(xy)), paste0, rownames(cov(xy))))
  )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          uw select   wg
## xx 9.166667    2.5  2.5
## xy 8.055556   -0.5 -0.5
## yx 8.055556   -0.5 -0.5
## yy 9.433333    1.7  1.7&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note how by weighting with a vector of 0 and 1s we are basically saying that the observations with a 0 will be excluded from the count.
They are weighted to have 0 impact on the computation of the descriptive statistics.
This is clear when you compare the results of the &lt;code&gt;select&lt;/code&gt; and &lt;code&gt;wg&lt;/code&gt; columns.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;computing-the-weighted-covariance-matrix-manually&#34; class=&#34;section level2&#34; number=&#34;2.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.2&lt;/span&gt; Computing the weighted covariance matrix manually&lt;/h2&gt;
&lt;p&gt;We could replicate the results of the weighting simply by selecting a subset of the original data because all observations were either weighted 0 or equally (1).
When this is not the case, weighting is slightly more complicated.&lt;/p&gt;
&lt;div id=&#34;exploring-the-statscov.wt-function-code&#34; class=&#34;section level3&#34; number=&#34;2.2.1&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.2.1&lt;/span&gt; Exploring the &lt;code&gt;stats::cov.wt()&lt;/code&gt; function code&lt;/h3&gt;
&lt;p&gt;Let’s look at how the &lt;code&gt;cov.wt()&lt;/code&gt; function works more in depth.
The internal code of the function is the following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Examine the internal code of stats::cov.wt() ---------------------------------

  cov.wt&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## function (x, wt = rep(1/nrow(x), nrow(x)), cor = FALSE, center = TRUE, 
##     method = c(&amp;quot;unbiased&amp;quot;, &amp;quot;ML&amp;quot;)) 
## {
##     if (is.data.frame(x)) 
##         x &amp;lt;- as.matrix(x)
##     else if (!is.matrix(x)) 
##         stop(&amp;quot;&amp;#39;x&amp;#39; must be a matrix or a data frame&amp;quot;)
##     if (!all(is.finite(x))) 
##         stop(&amp;quot;&amp;#39;x&amp;#39; must contain finite values only&amp;quot;)
##     n &amp;lt;- nrow(x)
##     if (with.wt &amp;lt;- !missing(wt)) {
##         if (length(wt) != n) 
##             stop(&amp;quot;length of &amp;#39;wt&amp;#39; must equal the number of rows in &amp;#39;x&amp;#39;&amp;quot;)
##         if (any(wt &amp;lt; 0) || (s &amp;lt;- sum(wt)) == 0) 
##             stop(&amp;quot;weights must be non-negative and not all zero&amp;quot;)
##         wt &amp;lt;- wt/s
##     }
##     if (is.logical(center)) {
##         center &amp;lt;- if (center) 
##             colSums(wt * x)
##         else 0
##     }
##     else {
##         if (length(center) != ncol(x)) 
##             stop(&amp;quot;length of &amp;#39;center&amp;#39; must equal the number of columns in &amp;#39;x&amp;#39;&amp;quot;)
##     }
##     x &amp;lt;- sqrt(wt) * sweep(x, 2, center, check.margin = FALSE)
##     cov &amp;lt;- switch(match.arg(method), unbiased = crossprod(x)/(1 - 
##         sum(wt^2)), ML = crossprod(x))
##     y &amp;lt;- list(cov = cov, center = center, n.obs = n)
##     if (with.wt) 
##         y$wt &amp;lt;- wt
##     if (cor) {
##         Is &amp;lt;- 1/sqrt(diag(cov))
##         R &amp;lt;- cov
##         R[] &amp;lt;- Is * cov * rep(Is, each = nrow(cov))
##         y$cor &amp;lt;- R
##     }
##     y
## }
## &amp;lt;bytecode: 0x7f8135ac48d0&amp;gt;
## &amp;lt;environment: namespace:stats&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The first thing to pay attention to is that the function can &lt;strong&gt;compute&lt;/strong&gt; the weighted covariance matrix &lt;strong&gt;in two ways&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;unbiased, using &lt;code&gt;corssprod(x) / (1 - sum(wt^2))&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;ML (or maximum likelihood), using &lt;code&gt;corssprod(x)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Note that the &lt;code&gt;wt&lt;/code&gt; object is divided by the sum of the values it is storing, which amounts to &lt;strong&gt;normalising&lt;/strong&gt; the weights.
This happens with &lt;code&gt;wt &amp;lt;- wt/s&lt;/code&gt; with &lt;code&gt;s&lt;/code&gt; being created inside an if statement as &lt;code&gt;s &amp;lt;- sum(wt)&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;x&lt;/code&gt; is &lt;strong&gt;centered&lt;/strong&gt; on the normalized weigthed means using the &lt;code&gt;sweep&lt;/code&gt; function&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;x&lt;/code&gt; is &lt;strong&gt;weighted&lt;/strong&gt; by multiplying by &lt;code&gt;sqrt(wt)&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;reproducing-the-internal-steps&#34; class=&#34;section level3&#34; number=&#34;2.2.2&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.2.2&lt;/span&gt; Reproducing the internal steps&lt;/h3&gt;
&lt;p&gt;First, we’ll &lt;strong&gt;set up&lt;/strong&gt; a few objects we need to replicate manually what happens inside the &lt;code&gt;stats::cov.wt()&lt;/code&gt; function.
We need to define a dataset, a vector of weights, a method to compute descriptives, and based on these we will also create an object to store the number of rows (&lt;code&gt;n&lt;/code&gt;).
As a vector of weights we sample random values between 0 and 1.
We can think of this as an attempt to weight each observation for the probability of sampling them from a population.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Set up manual computation of cov.wt() ----------------------------------------

  # Assign values to the function arguments
  x      &amp;lt;- xy                     # data
  set.seed(20220314)
  wi     &amp;lt;- runif(length(wi), min = 0, max = 1)
  method &amp;lt;- &amp;quot;ML&amp;quot;                   # use Maximum Likelihood for estimation

  # Assign values to some of the internal objects
  n &amp;lt;- nrow(x)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we want to make sure we &lt;strong&gt;normalize the weights&lt;/strong&gt;.
In other words we want to make sure the weights sum to 1.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Normalize weights ------------------------------------------------------------

  # Normalise weights (to sum to 1)
  wn &amp;lt;- wi / sum(wi)

  # Check they sum to 1
  sum(wn) == 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, we want to compute the &lt;strong&gt;vector of &lt;a href=&#34;https://en.wikipedia.org/wiki/Weighted_arithmetic_mean#:~:text=Mathematical%20definition%5Bedit%5D&#34;&gt;weighted means&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Compute the weighted means ---------------------------------------------------

  # Center on weighted mean if required
  center &amp;lt;- colSums(wn * x)

  # Center X on the weigthed mean
  x_cent &amp;lt;- sweep(x, 2, center, check.margin = FALSE)

  # Note that the sweep is subtracting the &amp;quot;center&amp;quot; to each value
  all.equal(
    sweep(x, 2, center, check.margin = FALSE),
    t(apply(x, 1, function (i) i - center))
  )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the weighted mean is computed as:
&lt;span class=&#34;math display&#34;&gt;\[
\bar{x} = \sum_{i = 1}^{n} w_i x_i
\]&lt;/span&gt;
and that &lt;code&gt;center &amp;lt;- colSums(wn * x)&lt;/code&gt; is doing exactly this.&lt;/p&gt;
&lt;p&gt;Finally, we want to compute the &lt;strong&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Sample_mean_and_covariance#:~:text=weighted%20covariance%20matrix&#34;&gt;weighted covariance matrix&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Compute the weighted covariance matrix ---------------------------------------

  # Weight (centered) data
  x_weighted &amp;lt;- sqrt(wn) * x_cent

  # Compute the ML weigthed covariance matrix manually
  covwt_man &amp;lt;- crossprod(x_weighted)

  # Print the manual weigthed covariance matrix
  covwt_man&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          x        y
## x 7.102015 5.431419
## y 5.431419 6.210209&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  # Compute the ML weigthed covariance matrix with stats::cov.wt()
  covwt_stats &amp;lt;- cov.wt(xy, wt = wi, method = &amp;quot;ML&amp;quot;, center = TRUE)$cov

  # Compare manual and stats weigthed covariance matrices
  covwt_man - covwt_stats&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   x y
## x 0 0
## y 0 0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;mathematical-formula-and-alternative-r-computations&#34; class=&#34;section level2&#34; number=&#34;2.3&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.3&lt;/span&gt; Mathematical formula and alternative R computations&lt;/h2&gt;
&lt;div id=&#34;unbiased-weighted-covariance-matrix&#34; class=&#34;section level3&#34; number=&#34;2.3.1&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.3.1&lt;/span&gt; Unbiased weighted covariance matrix&lt;/h3&gt;
&lt;p&gt;For a given population covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt;, each element &lt;span class=&#34;math inline&#34;&gt;\(q_{ik}\)&lt;/span&gt; of the &lt;strong&gt;unbiased&lt;/strong&gt; estimation of the weighted covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(\hat{Q}\)&lt;/span&gt; can be computed with the following formula:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
q_{ik} = \frac{1}{1 - \sum_{i = 1}^{n} w_i^2} \sum_{i = 1}^{n} w_i (x_{ij} - \bar{x}_j) (x_{ij} - \bar{x}_k)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;with &lt;span class=&#34;math inline&#34;&gt;\(\bar{x}_j\)&lt;/span&gt; being the weighted mean for variable &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(w_i\)&lt;/span&gt; being the normalized weight for a given observation (which we store in the vector &lt;code&gt;wn&lt;/code&gt;).
The following are alternative ways of computing it with mathematical or R shortcuts:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Alternative computations of the unbiased weighted covariance mat -------------

  # Literal translation of equation
  1 / (1 - sum(wn^2)) * t(wn * x_cent) %*% (x_cent)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          x        y
## x 8.320767 6.363485
## y 6.363485 7.275922&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  # Rearrange denominator
  t(wn * x_cent) %*% (x_cent) / (1 - sum(wn^2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          x        y
## x 8.320767 6.363485
## y 6.363485 7.275922&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  # Spread wn
  t(sqrt(wn) * x_cent) %*% (sqrt(wn)*x_cent) / (1 - sum(wn^2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          x        y
## x 8.320767 6.363485
## y 6.363485 7.275922&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  # Replace manual cross-product with R cross-product
  crossprod(sqrt(wn) * x_cent)/(1 - sum(wn^2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          x        y
## x 8.320767 6.363485
## y 6.363485 7.275922&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  # Compute with stats::cov.wt()
  cov.wt(xy, wt = wi, method = &amp;quot;unbiased&amp;quot;, center = TRUE)$cov&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          x        y
## x 8.320767 6.363485
## y 6.363485 7.275922&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;maximum-likelihood-weighted-covariance-matrix&#34; class=&#34;section level3&#34; number=&#34;2.3.2&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.3.2&lt;/span&gt; Maximum Likelihood weighted covariance matrix&lt;/h3&gt;
&lt;p&gt;Each element &lt;span class=&#34;math inline&#34;&gt;\(q_{ik}\)&lt;/span&gt; of the &lt;strong&gt;maximum likelihood&lt;/strong&gt; weighted covariance matrix estimate &lt;span class=&#34;math inline&#34;&gt;\(\hat{Q}\)&lt;/span&gt; can be computed manually with the following formula:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
q_{ik} = \frac{1}{\sum_{i = 1}^{n} w_i} \sum_{i = 1}^{n} w_i (x_{ij} - \bar{x}_j) (x_{ij} - \bar{x}_k)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;with &lt;span class=&#34;math inline&#34;&gt;\(\bar{x}_j\)&lt;/span&gt; being the weighted mean for variable &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(w_i\)&lt;/span&gt; being the normalized weight for a given observation.
The following are alternative ways of computing it with mathematical or R shortcuts:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Alternative computations of the ML weighted covariance mat -------------------

  # R manual cross-product using un-normalised weights
  1 / sum(wi) * t(wi * x_cent) %*% (x_cent)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          x        y
## x 7.102015 5.431419
## y 5.431419 6.210209&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  # Using the normalised weights
  1 / sum(wn) * t(wn * x_cent) %*% (x_cent)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          x        y
## x 7.102015 5.431419
## y 5.431419 6.210209&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  # Dropp the term = 1
  t(wn * x_cent) %*% (x_cent)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          x        y
## x 7.102015 5.431419
## y 5.431419 6.210209&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  # Spread wn
  t(sqrt(wn) * x_cent) %*% (sqrt(wn) * x_cent)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          x        y
## x 7.102015 5.431419
## y 5.431419 6.210209&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  # Replace manual cross-product with R cross-product
  crossprod(sqrt(wn) * x_cent)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          x        y
## x 7.102015 5.431419
## y 5.431419 6.210209&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  # R cross-product matrix
  crossprod(x_weighted)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          x        y
## x 7.102015 5.431419
## y 5.431419 6.210209&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  # Compute with stats::cov.wt()
  cov.wt(xy, wt = wi, method = &amp;quot;ML&amp;quot;, center = TRUE)$cov&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          x        y
## x 7.102015 5.431419
## y 5.431419 6.210209&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;relationship-with-the-matrix-of-sufficient-statistics&#34; class=&#34;section level2&#34; number=&#34;2.4&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.4&lt;/span&gt; Relationship with the matrix of sufficient statistics&lt;/h2&gt;
&lt;p&gt;Note the relationship between the covariance matrix and the matrix of sufficient statistics is the same as in the &lt;em&gt;unweighted&lt;/em&gt; case: &lt;span class=&#34;math inline&#34;&gt;\(\text{cov} = \frac{T_{\text{obs}}}{n}\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Obtain the matrix of sufficient statistics Tobs ------------------------------

  # Define a new weigthing object
  set.seed(20220314)
  wi &amp;lt;- runif(length(wi), min = 0, max = 1)
  wn &amp;lt;- wi / sum(wi)

  # Compute the weighted means of X again
  center &amp;lt;- colSums(wn * x)
  x_cent &amp;lt;- sweep(x, 2, center, check.margin = FALSE)

  # &amp;quot;Effective&amp;quot; sample size
  n &amp;lt;- sum(wi)

  # Number of columns
  p &amp;lt;- ncol(x)

  # Obtain matrix of sufficient statistics (Tobs)
  Tobs_lopp &amp;lt;- matrix(0, p, p)
  for(i in 1:nrow(x)){
    Tobs_lopp &amp;lt;- Tobs_lopp + wi[i] * (x_cent[i, ]) %*% t(x_cent[i, ])
  }

  # Obtain matrix of sufficient statistics (Tobs) w/ cross-product shortcut
  Tobs_cp &amp;lt;- t(wi * x_cent) %*% x_cent

  # Compare loop version and cross-product shortcut
  Tobs_lopp - Tobs_cp&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      x            y
## [1,] 0 3.552714e-15
## [2,] 0 0.000000e+00&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  # Assign simpler name and print Tobs
  (Tobs &amp;lt;- Tobs_cp)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          x        y
## x 31.08417 23.77229
## y 23.77229 27.18091&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  # Convert to a covariance matrix
  covmat &amp;lt;- Tobs / n

  # Check it&amp;#39;s what you were expecting
  covmat - cov.wt(xy, wt = wi, method = &amp;quot;ML&amp;quot;, center = TRUE)$cov&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   x             y
## x 0 -8.881784e-16
## y 0 -8.881784e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;we are using the normalized weights &lt;code&gt;wn&lt;/code&gt; to center the data, but we are using the un-normalised weights to scale the data contribution to &lt;code&gt;Tobs&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;if we had used the normalized weights, &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; would have been equal to 1 and &lt;code&gt;covmat&lt;/code&gt; would be equal to &lt;code&gt;Tobs&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Obtain the matrix of sufficient statistics Tobs (normalised weights) ---------

  # Convert to a covariance matrix
  covmat - t(wn * x_cent) %*% x_cent&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              x y
## x 0.000000e+00 0
## y 8.881784e-16 0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  # Then, covmat relates to Tobs as
  (t(wn * x_cent) %*% x_cent * n) - Tobs_cp&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               x y
## x  0.000000e+00 0
## y -3.552714e-15 0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  # So we could say
  Tobs &amp;lt;- t(wn * x_cent) %*% x_cent * n&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;tldr-just-give-me-the-code&#34; class=&#34;section level1&#34; number=&#34;3&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; TL;DR, just give me the code!&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Initial simple example -------------------------------------------------------

  # Get the dataset used in the example of stats::cov.wt()
  xy &amp;lt;- cbind(x = 1:10, y = c(1:3, 8:5, 8:10))

  # Define non-negative weights (as in example of stats::cov.wt())
  wi &amp;lt;- c(0,0,0,1,1,1,1,1,0,0)

  # Get the weighted estimate with the default methods
  covwt_stats &amp;lt;- stats::cov.wt(xy, wt = wi) # i.e. method = &amp;quot;unbiased&amp;quot;

  # Compare unweighted and weighted means
  data.frame(uw = colMeans(xy),
             select = colMeans(xy[wi == 1, ]),
             wg = covwt_stats$center)

  # Compare unweighted and weighted covariance matrix
  data.frame(uw = c(cov(xy)),
             select = c(cov(xy[wi == 1, ])),
             wg = c(covwt_stats$cov),
             row.names = c(sapply(colnames(cov(xy)), paste0, rownames(cov(xy))))
  )

# Examine the internal code of stats::cov.wt() ---------------------------------

  cov.wt

# Set up manual computation of cov.wt() ----------------------------------------

  # Assign values to the function arguments
  x      &amp;lt;- xy                     # data
  set.seed(20220314)
  wi     &amp;lt;- runif(length(wi), min = 0, max = 1)
  method &amp;lt;- &amp;quot;ML&amp;quot;                   # use Maximum Likelihood for estimation

  # Assign values to some of the internal objects
  n &amp;lt;- nrow(x)

# Normalize weights ------------------------------------------------------------

  # Normalise weights (to sum to 1)
  wn &amp;lt;- wi / sum(wi)

  # Check they sum to 1
  sum(wn) == 1


# Compute the weighted means ---------------------------------------------------

  # Center on weighted mean if required
  center &amp;lt;- colSums(wn * x)

  # Center X on the weigthed mean
  x_cent &amp;lt;- sweep(x, 2, center, check.margin = FALSE)

  # Note that the sweep is subtracting the &amp;quot;center&amp;quot; to each value
  all.equal(
    sweep(x, 2, center, check.margin = FALSE),
    t(apply(x, 1, function (i) i - center))
  )


# Compute the weighted covariance matrix ---------------------------------------

  # Weight (centered) data
  x_weighted &amp;lt;- sqrt(wn) * x_cent

  # Compute the ML weigthed covariance matrix manually
  covwt_man &amp;lt;- crossprod(x_weighted)

  # Print the manual weigthed covariance matrix
  covwt_man

  # Compute the ML weigthed covariance matrix with stats::cov.wt()
  covwt_stats &amp;lt;- cov.wt(xy, wt = wi, method = &amp;quot;ML&amp;quot;, center = TRUE)$cov

  # Compare manual and stats weigthed covariance matrices
  covwt_man - covwt_stats

# Alternative computations of the unbiased weighted covariance mat -------------

  # Literal translation of equation
  1 / (1 - sum(wn^2)) * t(wn * x_cent) %*% (x_cent)

  # Rearrange denominator
  t(wn * x_cent) %*% (x_cent) / (1 - sum(wn^2))

  # Spread wn
  t(sqrt(wn) * x_cent) %*% (sqrt(wn)*x_cent) / (1 - sum(wn^2))

  # Replace manual cross-product with R cross-product
  crossprod(sqrt(wn) * x_cent)/(1 - sum(wn^2))

  # Compute with stats::cov.wt()
  cov.wt(xy, wt = wi, method = &amp;quot;unbiased&amp;quot;, center = TRUE)$cov

# Alternative computations of the ML weighted covariance mat -------------------

  # R manual cross-product using un-normalised weights
  1 / sum(wi) * t(wi * x_cent) %*% (x_cent)

  # Using the normalised weights
  1 / sum(wn) * t(wn * x_cent) %*% (x_cent)

  # Dropp the term = 1
  t(wn * x_cent) %*% (x_cent)

  # Spread wn
  t(sqrt(wn) * x_cent) %*% (sqrt(wn) * x_cent)

  # Replace manual cross-product with R cross-product
  crossprod(sqrt(wn) * x_cent)

  # R cross-product matrix
  crossprod(x_weighted)

  # Compute with stats::cov.wt()
  cov.wt(xy, wt = wi, method = &amp;quot;ML&amp;quot;, center = TRUE)$cov

# Obtain the matrix of sufficient statistics Tobs ------------------------------

  # Define a new weigthing object
  set.seed(20220314)
  wi &amp;lt;- runif(length(wi), min = 0, max = 1)
  wn &amp;lt;- wi / sum(wi)

  # Compute the weighted means of X again
  center &amp;lt;- colSums(wn * x)
  x_cent &amp;lt;- sweep(x, 2, center, check.margin = FALSE)

  # &amp;quot;Effective&amp;quot; sample size
  n &amp;lt;- sum(wi)

  # Number of columns
  p &amp;lt;- ncol(x)

  # Obtain matrix of sufficient statistics (Tobs)
  Tobs_lopp &amp;lt;- matrix(0, p, p)
  for(i in 1:nrow(x)){
    Tobs_lopp &amp;lt;- Tobs_lopp + wi[i] * (x_cent[i, ]) %*% t(x_cent[i, ])
  }

  # Obtain matrix of sufficient statistics (Tobs) w/ cross-product shortcut
  Tobs_cp &amp;lt;- t(wi * x_cent) %*% x_cent

  # Compare loop version and cross-product shortcut
  Tobs_lopp - Tobs_cp

  # Assign simpler name and print Tobs
  (Tobs &amp;lt;- Tobs_cp)

  # Convert to a covariance matrix
  covmat &amp;lt;- Tobs / n

  # Check it&amp;#39;s what you were expecting
  covmat - cov.wt(xy, wt = wi, method = &amp;quot;ML&amp;quot;, center = TRUE)$cov

# Obtain the matrix of sufficient statistics Tobs (normalised weights) ---------

  # Convert to a covariance matrix
  covmat - t(wn * x_cent) %*% x_cent

  # Then, covmat relates to Tobs as
  (t(wn * x_cent) %*% x_cent * n) - Tobs_cp

  # So we could say
  Tobs &amp;lt;- t(wn * x_cent) %*% x_cent * n&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Estimating ridge regression in R</title>
      <link>https://edoardocostantini.github.io/post/ridge/</link>
      <pubDate>Mon, 28 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://edoardocostantini.github.io/post/ridge/</guid>
      <description>
&lt;script src=&#34;https://edoardocostantini.github.io/post/ridge/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1&lt;/span&gt; Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#learn-by-coding&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2&lt;/span&gt; Learn by coding&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#fitting-ridge-regression-manually&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.1&lt;/span&gt; Fitting ridge regression manually&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#an-alternative-way-to-avoid-penalising-the-intercept&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.1.1&lt;/span&gt; An alternative way to avoid penalising the intercept&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#fit-ridge-regression-with-glmnet&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.2&lt;/span&gt; Fit ridge regression with glmnet&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#use-the-biased-estimation-of-variance&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.2.1&lt;/span&gt; Use the biased estimation of variance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#return-the-unstandardized-coefficients&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.2.2&lt;/span&gt; Return the unstandardized coefficients&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#adjust-the-parametrization-of-lambda-for-glmnet&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.2.3&lt;/span&gt; Adjust the parametrization of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; for &lt;code&gt;glmnet&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#compare-manual-and-glmnet-ridge-regression-output&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.2.4&lt;/span&gt; Compare manual and &lt;code&gt;glmnet&lt;/code&gt; ridge regression output&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tldr-just-give-me-the-code&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3&lt;/span&gt; TL;DR, just give me the code!&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34; number=&#34;1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Introduction&lt;/h1&gt;
&lt;p&gt;When there are many correlated predictors in a linear regression model, their regression coefficients can become poorly determined and exhibit high variance.
This problem can be alleviated by imposing a size constraint (or penalty) on the coefficients.
Ridge regression shrinks the regression coefficients by imposing a penalty on their size.
The ridge coefficients values minimize a penalized residual sum of squares:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{\beta}^{\text{ridge}} = \text{argmin}_{\beta} \left\{ \sum_{i=1}^{N} \left( y_i - \beta_0 - \sum_{j=1}^{p} x_{ij}\beta_j \right)^2 + \lambda \sum_{j=1}^{p}\beta_j^2 \right\}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The ridge solutions are not equivariant under scaling of the inputs.
Therefore, it is recommended to standardize the inputs before solving the minimization problem.&lt;/p&gt;
&lt;p&gt;Notice that the intercept &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; has been left out of the penalty term.
Penalization of the intercept would make the procedure depend on the origin chosen for &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;.
Furthermore, by centering the predictors, we can separate the solution to the &lt;a href=&#34;https://www.notion.so/Ridge-regression-8134d8babda5413ab182df645c6196a8&#34;&gt;minimazion problem&lt;/a&gt; into two parts:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Intercept
&lt;span class=&#34;math display&#34;&gt;\[
\hat{\beta}_0 = \bar{y}=\frac{1}{N}\sum_{i = 1}^{N} y_i
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Penalised regression coefficients
&lt;span class=&#34;math display&#34;&gt;\[
\hat{\beta}^{\text{ridge}}=(\mathbf{X}^T\mathbf{X} + \lambda \mathbf{I})^{-1}\mathbf{X}^Ty
\]&lt;/span&gt;
which is the regular way of estimating regression coefficients with a penalty term (&lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;) added on the diagonal (&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{I}\)&lt;/span&gt;) of the cross-product matrix (&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}^T\mathbf{X}\)&lt;/span&gt;) to make it invertible (&lt;span class=&#34;math inline&#34;&gt;\((...)^{-1}\)&lt;/span&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;learn-by-coding&#34; class=&#34;section level1&#34; number=&#34;2&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Learn by coding&lt;/h1&gt;
&lt;p&gt;The &lt;code&gt;glmnet&lt;/code&gt; package can be used to obtain the ridge regression estimates of the regression coefficients.
In this section, we will first see how to obtain these estimates “manually”, that is coding every step on our own, and then we will see how to obtain the same results using the &lt;code&gt;glmnet&lt;/code&gt; package.&lt;/p&gt;
&lt;p&gt;Let’s start by setting up the R environment.
In this post, we will work with the &lt;code&gt;mtcars&lt;/code&gt; data.
If you are not familiar with it, just look up the R help file on it.
We will use the first column of the dataset (variable named &lt;code&gt;mpg&lt;/code&gt;) as a dependent variable and the remaining ones as predictors in a linear regression.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Set up -----------------------------------------------------------------------

# Load packages
library(glmnet)

# Take the mtcars data
y &amp;lt;- mtcars[, &amp;quot;mpg&amp;quot;]
X &amp;lt;- mtcars[, -1]

# Create a few shorthands we will use
n &amp;lt;- nrow(X)
p &amp;lt;- ncol(X)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;fitting-ridge-regression-manually&#34; class=&#34;section level2&#34; number=&#34;2.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.1&lt;/span&gt; Fitting ridge regression manually&lt;/h2&gt;
&lt;p&gt;First, let’s make sure the predictors are centered on the mean and scaled to have a variance of 1.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Fitting ridge regression manually --------------------------------------------

# Scale the data (standardize)
X_scale &amp;lt;- scale(X, center = TRUE, scale = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, we want to &lt;strong&gt;fit the ridge regression&lt;/strong&gt; manually by separating the intercept and the regression coefficients estimation (two-step approach):&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Estimate the intercept (&lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_0\)&lt;/span&gt;)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Estimate the intercept
b0_hat_r &amp;lt;- mean(y)&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Estimate the ridge regression coefficients (&lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}^{\text{ridge}}\)&lt;/span&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;p&gt;Compute the cross-product matrix of the predictors.&lt;/p&gt;
&lt;p&gt;This is the same step we would take if we wanted to compute the OLS estimates.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Compute the cross-product matrix of the data
XtX &amp;lt;- t(X_scale) %*% X_scale&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Define a value of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;This value is usually chosen by cross-validation from a grid of possible values.
However, here we are only interested in how &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; is used in the computation, so we can simply give it a fixed value.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Define a lambda value
lambda &amp;lt;- .1&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Compute &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}^{\text{ridge}}\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Estimate the regression coefficients with the ridge penalty
bs_hat_r &amp;lt;- solve(XtX + lambda * diag(p)) %*% t(X_scale) %*% y&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where &lt;code&gt;diag(p)&lt;/code&gt; is the identity matrix &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{I}\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Finally, let’s print the results:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Print the results
round(
  data.frame(twostep = c(b0 = b0_hat_r,
                         b = bs_hat_r)),
  3
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     twostep
## b0   20.091
## b1   -0.194
## b2    1.366
## b3   -1.373
## b4    0.438
## b5   -3.389
## b6    1.361
## b7    0.162
## b8    1.243
## b9    0.496
## b10  -0.460&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is important to note the effect of centering and scaling.
When fitting ridge regression, many sources recommend centering the data.
This allows to separate the estimation of the intercept from the estimation of the regression coefficients.
As a result, only the regression coefficients are penalised.
To understand the effect of centering, consider what happens in regular OLS estimation when &lt;strong&gt;predictors are centered&lt;/strong&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Centering in regular OLS -----------------------------------------------------

# Create a version of X that is centered
X_center &amp;lt;- scale(X, center = TRUE, scale = FALSE)

# Fit an regular linear model
lm_ols &amp;lt;- lm(y ~ X_center)

# Check that b0 is equal to the mean of y
coef(lm_ols)[&amp;quot;(Intercept)&amp;quot;] - mean(y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   (Intercept) 
## -3.552714e-15&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Furthermore, let’s see what would have happened if we had penalised the intercept as well.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Consequence of penalising the intercept --------------------------------------

# Add a vector of 1s to penalise the intercept
X_scale_w1 &amp;lt;- cbind(1, X_scale)

# Compute the cross-product matrix of the data
XtX &amp;lt;- t(X_scale_w1) %*% X_scale_w1

# Estimate the regression coefficients with the ridge penalty
bs_hat_r_w1 &amp;lt;- solve(XtX + lambda * diag(p+1)) %*% t(X_scale_w1) %*% y

# Print the results
round(
  data.frame(twostep = c(b0 = b0_hat_r,
                         b = bs_hat_r),
             onestep = c(b0 = bs_hat_r_w1[1],
                         b = bs_hat_r_w1[-1])),
  3
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     twostep onestep
## b0   20.091  20.028
## b1   -0.194  -0.194
## b2    1.366   1.366
## b3   -1.373  -1.373
## b4    0.438   0.438
## b5   -3.389  -3.389
## b6    1.361   1.361
## b7    0.162   0.162
## b8    1.243   1.243
## b9    0.496   0.496
## b10  -0.460  -0.460&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you see, the intercept would be shrunk toward zero, without any benefit.
As a result, any prediction would also be offset by the same amount.&lt;/p&gt;
&lt;div id=&#34;an-alternative-way-to-avoid-penalising-the-intercept&#34; class=&#34;section level3&#34; number=&#34;2.1.1&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.1.1&lt;/span&gt; An alternative way to avoid penalising the intercept&lt;/h3&gt;
&lt;p&gt;It can be handy to obtain estimates of the regression coefficients and intercept in one step.
We can use matrix algebra and R to simplify the two-step procedure to a single step.
In particular, we can avoid the penalisation of the intercept by setting to 0 the first element of the “penalty” matrix &lt;code&gt;lambda * diag(p + 1)&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Alternative to avoid penalization of the intercept ---------------------------

# Compute cross-product matrix
XtX &amp;lt;- crossprod(X_scale_w1)

# Create penalty matrix
pen &amp;lt;- lambda * diag(p + 1)

# replace first element with 0
pen[1, 1] &amp;lt;- 0

# Obtain standardized estimates
bs_hat_r2 &amp;lt;- solve(XtX + pen) %*% t(X_scale_w1) %*% (y)

# Compare
round(
        data.frame(
                twostep = c(b0 = b0_hat_r, b = bs_hat_r),
                onestep = c(b0 = bs_hat_r2[1], b = bs_hat_r2[-1])
        ),
        3
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     twostep onestep
## b0   20.091  20.091
## b1   -0.194  -0.194
## b2    1.366   1.366
## b3   -1.373  -1.373
## b4    0.438   0.438
## b5   -3.389  -3.389
## b6    1.361   1.361
## b7    0.162   0.162
## b8    1.243   1.243
## b9    0.496   0.496
## b10  -0.460  -0.460&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-ridge-regression-with-glmnet&#34; class=&#34;section level2&#34; number=&#34;2.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.2&lt;/span&gt; Fit ridge regression with glmnet&lt;/h2&gt;
&lt;p&gt;The most popular R package to fit regularised regression is &lt;code&gt;glmnet&lt;/code&gt;.
Let’s see how we can replicate the results we obtained with the manual approach with glmnet.
There are three important differences to consider:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;glmnet&lt;/code&gt; uses the &lt;a href=&#34;https://en.wikipedia.org/wiki/Variance#Biased_sample_variance&#34;&gt;biased sample variance estimate&lt;/a&gt; when scaling the predictors;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;glmnet&lt;/code&gt; returns the unstandardized regression coefficients;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;glmnet&lt;/code&gt; uses a different parametrization for &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To obtain the same results with the manual approach and &lt;code&gt;glmnet&lt;/code&gt; we need to account for these.&lt;/p&gt;
&lt;div id=&#34;use-the-biased-estimation-of-variance&#34; class=&#34;section level3&#34; number=&#34;2.2.1&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.2.1&lt;/span&gt; Use the biased estimation of variance&lt;/h3&gt;
&lt;p&gt;First, let’s use the biased sample variance estimate in computing &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}^{\text{ridge}}\)&lt;/span&gt; with the manual approach:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Fitting ridge manually with biased variance estimation -----------------------

# Standardize X
X_scale &amp;lt;- sapply(1:p, function (j){
  muj &amp;lt;- mean(X[, j])                  # mean
  sj &amp;lt;- sqrt( var(X[, j]) * (n-1) / n) # (biased) sd
  (X[, j] - muj) / sj                  # center and scale
})

# Craete the desing matrix
X_scale_dm &amp;lt;- cbind(1, X_scale)

# Compute cross-product matrix
XtX &amp;lt;- crossprod(X_scale_dm)

# Create penalty matrix
pen &amp;lt;- lambda * diag(p + 1)
pen[1, 1] &amp;lt;- 0

# Obtain standardized estimates
bs_hat_r3 &amp;lt;- solve(XtX + pen) %*% t(X_scale_dm) %*% (y)

# Print results
round(
      data.frame(
              manual = c(b0 = bs_hat_r3[1], b = bs_hat_r3[-1])
      ),
      3
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     manual
## b0  20.091
## b1  -0.191
## b2   1.353
## b3  -1.354
## b4   0.430
## b5  -3.343
## b6   1.343
## b7   0.159
## b8   1.224
## b9   0.488
## b10 -0.449&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;return-the-unstandardized-coefficients&#34; class=&#34;section level3&#34; number=&#34;2.2.2&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.2.2&lt;/span&gt; Return the unstandardized coefficients&lt;/h3&gt;
&lt;p&gt;Next, we need to revert these regression coefficients to their original scale.
Since we are estimating the regression coefficients on the scaled data, they are computed on the standardized scale.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Return the  unstandardized coefficients --------------------------------------

# Extract the original mean and standard deviations of all X variables
mean_x &amp;lt;- colMeans(X)
sd_x &amp;lt;- sqrt(apply(X, 2, var) * (n - 1) / n) # biased version

# Revert to original scale
bs_hat_r4 &amp;lt;- c(bs_hat_r3[1] - crossprod(mean_x, bs_hat_r3[-1] / sd_x),
               bs_hat_r3[-1] / sd_x)

# Compare manual standardized and unstandardized results
round(
      data.frame(
              standardized = c(b0 = bs_hat_r3[1], b = bs_hat_r3[-1]),
              unstandardized = c(b0 = bs_hat_r4[1], b = bs_hat_r4[-1])
      ),
      3
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     standardized unstandardized
## b0        20.091         12.908
## b1        -0.191         -0.109
## b2         1.353          0.011
## b3        -1.354         -0.020
## b4         0.430          0.818
## b5        -3.343         -3.471
## b6         1.343          0.764
## b7         0.159          0.320
## b8         1.224          2.491
## b9         0.488          0.672
## b10       -0.449         -0.282&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;adjust-the-parametrization-of-lambda-for-glmnet&#34; class=&#34;section level3&#34; number=&#34;2.2.3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.2.3&lt;/span&gt; Adjust the parametrization of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; for &lt;code&gt;glmnet&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;Next, we need to understand the relationship between the &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; parametrization we used and the one used by &lt;code&gt;glmnet&lt;/code&gt;.
The following code shows that if we want to use a given value of &lt;code&gt;lambda&lt;/code&gt; in &lt;code&gt;glmnet&lt;/code&gt; we need to multiply it by the standard deviation of the dependent variable (&lt;code&gt;sd_y&lt;/code&gt;) and divide it by the sample size (&lt;code&gt;n&lt;/code&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Adjust the parametrization of lambda -----------------------------------------

# Extract the original mean and standard deviations of y (for lambda parametrization)
mean_y &amp;lt;- mean(y)
sd_y &amp;lt;- sqrt(var(y) * (n - 1) / n)

# Compute the value glmnet wants for your target lambda
lambda_glmnet &amp;lt;- sd_y * lambda / n&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;compare-manual-and-glmnet-ridge-regression-output&#34; class=&#34;section level3&#34; number=&#34;2.2.4&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.2.4&lt;/span&gt; Compare manual and &lt;code&gt;glmnet&lt;/code&gt; ridge regression output&lt;/h3&gt;
&lt;p&gt;Finally, we can compare the results:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Fitting ridge regression with glmnet -----------------------------------------

# Fit glmnet
fit_glmnet_s &amp;lt;- glmnet(x = X,
                       y = y,
                       alpha = 0,
                       lambda = lambda_glmnet, # correction for how penalty is used
                       thresh = 1e-20)

bs_glmnet &amp;lt;- coef(fit_glmnet_s)

# Compare estimated coefficients
round(
      data.frame(
        manual = c(b0 = bs_hat_r4[1], b = bs_hat_r4[-1]),
        glmnet = c(b0 = bs_glmnet[1], b = bs_glmnet[-1])
      ),
      3
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        manual glmnet
## b0     12.908 12.908
## b.cyl  -0.109 -0.109
## b.disp  0.011  0.011
## b.hp   -0.020 -0.020
## b.drat  0.818  0.818
## b.wt   -3.471 -3.471
## b.qsec  0.764  0.764
## b.vs    0.320  0.320
## b.am    2.491  2.491
## b.gear  0.672  0.672
## b.carb -0.282 -0.282&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;tldr-just-give-me-the-code&#34; class=&#34;section level1&#34; number=&#34;3&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; TL;DR, just give me the code!&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Set up -----------------------------------------------------------------------

# Load packages
library(glmnet)

# Take the mtcars data
y &amp;lt;- mtcars[, &amp;quot;mpg&amp;quot;]
X &amp;lt;- mtcars[, -1]

# Create a few shorthands we will use
n &amp;lt;- nrow(X)
p &amp;lt;- ncol(X)

# Fitting ridge regression manually --------------------------------------------

# Scale the data (standardize)
X_scale &amp;lt;- scale(X, center = TRUE, scale = TRUE)

# Estimate the intercept
b0_hat_r &amp;lt;- mean(y)


# Compute the cross-product matrix of the data
XtX &amp;lt;- t(X_scale) %*% X_scale

# Define a lambda value
lambda &amp;lt;- .1

# Estimate the regression coefficients with the ridge penalty
bs_hat_r &amp;lt;- solve(XtX + lambda * diag(p)) %*% t(X_scale) %*% y

# Print the results
round(
  data.frame(twostep = c(b0 = b0_hat_r,
                         b = bs_hat_r)),
  3
)

# Centering in regular OLS -----------------------------------------------------

# Create a version of X that is centered
X_center &amp;lt;- scale(X, center = TRUE, scale = FALSE)

# Fit an regular linear model
lm_ols &amp;lt;- lm(y ~ X_center)

# Check that b0 is equal to the mean of y
coef(lm_ols)[&amp;quot;(Intercept)&amp;quot;] - mean(y)

# Consequence of penalising the intercept --------------------------------------

# Add a vector of 1s to penalise the intercept
X_scale_w1 &amp;lt;- cbind(1, X_scale)

# Compute the cross-product matrix of the data
XtX &amp;lt;- t(X_scale_w1) %*% X_scale_w1

# Estimate the regression coefficients with the ridge penalty
bs_hat_r_w1 &amp;lt;- solve(XtX + lambda * diag(p+1)) %*% t(X_scale_w1) %*% y

# Print the results
round(
  data.frame(twostep = c(b0 = b0_hat_r,
                         b = bs_hat_r),
             onestep = c(b0 = bs_hat_r_w1[1],
                         b = bs_hat_r_w1[-1])),
  3
)

# Alternative to avoid penalization of the intercept ---------------------------

# Compute cross-product matrix
XtX &amp;lt;- crossprod(X_scale_w1)

# Create penalty matrix
pen &amp;lt;- lambda * diag(p + 1)

# replace first element with 0
pen[1, 1] &amp;lt;- 0

# Obtain standardized estimates
bs_hat_r2 &amp;lt;- solve(XtX + pen) %*% t(X_scale_w1) %*% (y)

# Compare
round(
        data.frame(
                twostep = c(b0 = b0_hat_r, b = bs_hat_r),
                onestep = c(b0 = bs_hat_r2[1], b = bs_hat_r2[-1])
        ),
        3
)

# Fitting ridge manually with biased variance estimation -----------------------

# Standardize X
X_scale &amp;lt;- sapply(1:p, function (j){
  muj &amp;lt;- mean(X[, j])                  # mean
  sj &amp;lt;- sqrt( var(X[, j]) * (n-1) / n) # (biased) sd
  (X[, j] - muj) / sj                  # center and scale
})

# Craete the desing matrix
X_scale_dm &amp;lt;- cbind(1, X_scale)

# Compute cross-product matrix
XtX &amp;lt;- crossprod(X_scale_dm)

# Create penalty matrix
pen &amp;lt;- lambda * diag(p + 1)
pen[1, 1] &amp;lt;- 0

# Obtain standardized estimates
bs_hat_r3 &amp;lt;- solve(XtX + pen) %*% t(X_scale_dm) %*% (y)

# Print results
round(
      data.frame(
              manual = c(b0 = bs_hat_r3[1], b = bs_hat_r3[-1])
      ),
      3
)

# Return the  unstandardized coefficients --------------------------------------

# Extract the original mean and standard deviations of all X variables
mean_x &amp;lt;- colMeans(X)
sd_x &amp;lt;- sqrt(apply(X, 2, var) * (n - 1) / n) # biased version

# Revert to original scale
bs_hat_r4 &amp;lt;- c(bs_hat_r3[1] - crossprod(mean_x, bs_hat_r3[-1] / sd_x),
               bs_hat_r3[-1] / sd_x)

# Compare manual standardized and unstandardized results
round(
      data.frame(
              standardized = c(b0 = bs_hat_r3[1], b = bs_hat_r3[-1]),
              unstandardized = c(b0 = bs_hat_r4[1], b = bs_hat_r4[-1])
      ),
      3
)

# Adjust the parametrization of lambda -----------------------------------------

# Extract the original mean and standard deviations of y (for lambda parametrization)
mean_y &amp;lt;- mean(y)
sd_y &amp;lt;- sqrt(var(y) * (n - 1) / n)

# Compute the value glmnet wants for your target lambda
lambda_glmnet &amp;lt;- sd_y * lambda / n

# Fitting ridge regression with glmnet -----------------------------------------

# Fit glmnet
fit_glmnet_s &amp;lt;- glmnet(x = X,
                       y = y,
                       alpha = 0,
                       lambda = lambda_glmnet, # correction for how penalty is used
                       thresh = 1e-20)

bs_glmnet &amp;lt;- coef(fit_glmnet_s)

# Compare estimated coefficients
round(
      data.frame(
        manual = c(b0 = bs_hat_r4[1], b = bs_hat_r4[-1]),
        glmnet = c(b0 = bs_glmnet[1], b = bs_glmnet[-1])
      ),
      3
)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The sweep operator</title>
      <link>https://edoardocostantini.github.io/post/sweep/</link>
      <pubDate>Wed, 17 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://edoardocostantini.github.io/post/sweep/</guid>
      <description>
&lt;script src=&#34;https://edoardocostantini.github.io/post/sweep/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1&lt;/span&gt; Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#learn-by-coding&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2&lt;/span&gt; Learn by coding&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#coding-a-sweep-function-in-r&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.1&lt;/span&gt; Coding a sweep function in R&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#using-the-sweep-operator-to-estimate-regression-models&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.2&lt;/span&gt; Using the sweep operator to estimate regression models&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#compute-the-augmented-covariance-matrix&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.2.1&lt;/span&gt; Compute the augmented covariance matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#estimate-multivariate-linear-models&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.2.2&lt;/span&gt; Estimate multivariate linear models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tldr-just-give-me-the-code&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3&lt;/span&gt; TL;DR, just give me the code!&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34; number=&#34;1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Introduction&lt;/h1&gt;
&lt;p&gt;The sweep operator is a matrix transformation commonly used to estimate regression models.
It performs elementary row operations on a &lt;span class=&#34;math inline&#34;&gt;\(p \times p\)&lt;/span&gt; matrix which happen to be particularly useful to estimate multivariate linear models.
Little and Rubin &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-littleRubin:2002&#34; role=&#34;doc-biblioref&#34;&gt;2002, p148&lt;/a&gt;)&lt;/span&gt; defined it as follows:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The sweep operator is defined for symmetric matrices as follows. A &lt;span class=&#34;math inline&#34;&gt;\(p \times p\)&lt;/span&gt; symmetric matrix G is said to be swept on row and column k if it is replaced by another symmetric &lt;span class=&#34;math inline&#34;&gt;\(p \times p\)&lt;/span&gt; matrix H with elements defined as follows:
&lt;span class=&#34;math display&#34;&gt;\[
h_{kk} = -1/g_{kk}
\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[
h_{jk} = h_{kj} = \frac{g_{jk}}{g_{kk}}, j \neq k
\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[
h_{jl} = g_{jl} - \frac{g_{jk}g_{kl}}{g_{kk}}, j \neq k, l \neq k
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The notation indicating this transformation is usually a variation of &lt;span class=&#34;math inline&#34;&gt;\(\text{SWEEP}(k)[G]\)&lt;/span&gt;, which can be read as sweeping matrix &lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt; on row and column &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;.
The notation &lt;span class=&#34;math inline&#34;&gt;\(\text{SWEEP}(1, 2, ..., K)[G]\)&lt;/span&gt; indicates successive applications of &lt;span class=&#34;math inline&#34;&gt;\(\text{SWEEP}(k)[G]\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(k = 1, \dots, K\)&lt;/span&gt;.
Sweeps on multiple positions do not need to be carried out in any particular order.
The sweep operator is commutative.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{SWEEP}(k_2)[\text{SWEEP}(k_1)[G]] = \text{SWEEP}(k_1)[\text{SWEEP}(k_2)[G]]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In this post, I’m interested in exploring how we use the sweep operator to estimate the parameters of regressions models.
If you are interested in the mathematical details, I recommend reading the full sweep operator description in one of the following resources: Goodnight &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-goodnight:1979&#34; role=&#34;doc-biblioref&#34;&gt;1979, p154&lt;/a&gt;)&lt;/span&gt;, Schafer &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-schafer:1997&#34; role=&#34;doc-biblioref&#34;&gt;1997&lt;/a&gt;)&lt;/span&gt;, Little and Rubin &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-littleRubin:2002&#34; role=&#34;doc-biblioref&#34;&gt;2002, p148&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Goodnight &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-goodnight:1979&#34; role=&#34;doc-biblioref&#34;&gt;1979, p150&lt;/a&gt;)&lt;/span&gt; is a particularly helpful paper as it describes an easy to implement algorithm to perform the sweep operator.
Following Goodnight, given an originally symmetric positive definite matrix G, &lt;span class=&#34;math inline&#34;&gt;\(\text{SWEEP}(k)[G]\)&lt;/span&gt; modifies a matrix G as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Step 1: Let &lt;span class=&#34;math inline&#34;&gt;\(D = g_{kk}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Step 2: Divide row &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; by &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Step 3: For every other row &lt;span class=&#34;math inline&#34;&gt;\(i \neq k\)&lt;/span&gt;, let &lt;span class=&#34;math inline&#34;&gt;\(B = g_{ik}\)&lt;/span&gt;. Subtract &lt;span class=&#34;math inline&#34;&gt;\(B \times \text{row } k\)&lt;/span&gt; from row &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;. Set &lt;span class=&#34;math inline&#34;&gt;\(g_{ik} = -B/D\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Step 4: Set &lt;span class=&#34;math inline&#34;&gt;\(g_{kk} = 1/D\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;learn-by-coding&#34; class=&#34;section level1&#34; number=&#34;2&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Learn by coding&lt;/h1&gt;
&lt;div id=&#34;coding-a-sweep-function-in-r&#34; class=&#34;section level2&#34; number=&#34;2.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.1&lt;/span&gt; Coding a sweep function in R&lt;/h2&gt;
&lt;p&gt;Let’s start by coding a simple function that performs the operations described by Goodnight &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-goodnight:1979&#34; role=&#34;doc-biblioref&#34;&gt;1979, p150&lt;/a&gt;)&lt;/span&gt;.
We want a function that takes as inputs a symmetric matrix (argument &lt;code&gt;G&lt;/code&gt;) and a vector of positions to sweep over (argument &lt;code&gt;K&lt;/code&gt;).
The function below takes these two inputs and performs the four sweep steps for every element of &lt;code&gt;K&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Write an R function implementing SWEEP(k)[G] according to Goodnight ----------

sweepGoodnight &amp;lt;- function (G, K){

  for(k in K){
    # Step 1: Let D = g_kk
    D &amp;lt;- G[k, k]

    # Step 2: Divide row k by D.
    G[k, ] &amp;lt;- G[k, ] / D

    # Step 3:
    # - For every other row i != k, let B = g_ik
    # - Subtract B \times row k from row i.
    # - set g_ik = -B/D.
    for(i in 1:nrow(G)){
      if(i != k){
        B &amp;lt;- G[i, k]
        G[i, ] &amp;lt;- G[i, ] - B * G[k, ]
        G[i, k] &amp;lt;- -1 * B / D
      }
    }
    # Step 4: Set g_kk = 1/D
    G[k, k] = 1/D
  }

  # Output
  return(G)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s check that this function returns what we want by comparing it with a function implemented by someone else.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Compare sweepGoodnight with other implementations ----------------------------

# Install the `fastmatrix` package (run if you don&amp;#39;t have it yet)
# install.packages(&amp;quot;fastmatrix&amp;quot;)

# Load fastmatrix
library(fastmatrix)

# Define an example dataset
X &amp;lt;- matrix(c(1, 1, 1, 1,
              1, 2, 1, 3,
              1, 3, 1, 3,
              1, 1,-1, 2,
              1, 2,-1, 2,
              1, 3,-1, 1), ncol = 4, byrow = TRUE)

# Define the G matrix
G &amp;lt;- crossprod(X)

# Define a vector of positions to sweep over
K &amp;lt;- 1:3

# Perform SWEEP[K](G) with fastmatrix sweep.operator
H_fm &amp;lt;- sweep.operator(G, k = K)

# Perform SWEEP[K](G) with our sweepGoodnight implementation
H_sg &amp;lt;- sweepGoodnight(G, K = K)

# Compare the two
all.equal(H_fm, H_sg)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The functions &lt;code&gt;fastmatrix::sweep.operator()&lt;/code&gt; and &lt;code&gt;sweepGoodnight()&lt;/code&gt; return the same &lt;code&gt;H&lt;/code&gt; matrix by sweeping matrix &lt;code&gt;G&lt;/code&gt; over the positions defined in &lt;code&gt;K&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;using-the-sweep-operator-to-estimate-regression-models&#34; class=&#34;section level2&#34; number=&#34;2.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.2&lt;/span&gt; Using the sweep operator to estimate regression models&lt;/h2&gt;
&lt;p&gt;To understand how the sweep operator relates to the estimation of multivariate linear models, we will work with a data set used by Little and Rubin &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-littleRubin:2002&#34; role=&#34;doc-biblioref&#34;&gt;2002, p152&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Load Little Rubin data -------------------------------------------------------

# Create data
  X &amp;lt;- as.data.frame(
          matrix(
                  data = c(7, 1, 11, 11, 7, 11, 3, 1, 2, 21, 1, 11, 10, 26,
                           29, 56, 31, 52, 55, 71 ,31, 54, 47, 40, 66, 68,
                           6, 15, 8, 8, 6, 9, 17, 22, 18, 4, 23, 9, 8,
                           60, 52, 20, 47, 33, 22,6,44,22,26,34,12,12,
                           78.5, 74.3, 104.3, 87.6, 95.9, 109.2, 102.7,
                           72.5, 93.1, 115.9, 83.8, 113.3, 109.4),
                  ncol = 5
          )
  )

# Store useful information
  n &amp;lt;- nrow(X)
  p &amp;lt;- ncol(X)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take a quick look at the first rows of the data to get an idea of what we are working with.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Glance at the first 6 rows of the data
  head(X)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   V1 V2 V3 V4    V5
## 1  7 26  6 60  78.5
## 2  1 29 15 52  74.3
## 3 11 56  8 20 104.3
## 4 11 31  8 47  87.6
## 5  7 52  6 33  95.9
## 6 11 55  9 22 109.2&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;compute-the-augmented-covariance-matrix&#34; class=&#34;section level3&#34; number=&#34;2.2.1&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.2.1&lt;/span&gt; Compute the augmented covariance matrix&lt;/h3&gt;
&lt;p&gt;To obtain the estimates of the regression coefficients of a multivariate linear model, we need to sweep over the positions of the predictors the augmented covariance matrix of the data (&lt;span class=&#34;math inline&#34;&gt;\(\Theta\)&lt;/span&gt;).
This is a &lt;span class=&#34;math inline&#34;&gt;\((p+1) \times (p+1)\)&lt;/span&gt; matrix storing the covariance matrix and the means of the dataset.
It usually looks like this:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\Theta =
\begin{bmatrix}
1 &amp;amp; \mu_1 &amp;amp; ... &amp;amp;\mu_p\\
\mu_1 &amp;amp; \sigma^2_1 &amp;amp; ... &amp;amp; \sigma_{1p}\\
... &amp;amp; ... &amp;amp; ... &amp;amp; ...\\
\mu_p &amp;amp; \sigma_{1p} &amp;amp; ... &amp;amp; \sigma^2_{p}
\end{bmatrix}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;with &lt;span class=&#34;math inline&#34;&gt;\(\mu_1, \dots, \mu_p\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_1, \dots, \sigma^2_p\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{jk}\)&lt;/span&gt; being the means, variances, and covariances of the variables in our dataset, respectively.&lt;/p&gt;
&lt;p&gt;In R, we can obtain this matrix in just a few steps starting from our dataset &lt;code&gt;X&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Augment the original data&lt;/strong&gt; with a column of 1s on the left.&lt;/p&gt;
&lt;p&gt;We can use the &lt;code&gt;cbind()&lt;/code&gt; function to append a column of 1s to the left of X.
Keep in mind that we need to perform matrix operations with the resulting object.
Therefore, we need to make sure we are working with an R object of the class &lt;code&gt;matrix&lt;/code&gt; instead of &lt;code&gt;data.frame&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Obtain the augmented covariance matrix ---------------------------------------

# Augment X
  X_aug &amp;lt;- cbind(int = 1, as.matrix(X))

# Glance at the first 6 rows of X_aug
  head(X_aug)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      int V1 V2 V3 V4    V5
## [1,]   1  7 26  6 60  78.5
## [2,]   1  1 29 15 52  74.3
## [3,]   1 11 56  8 20 104.3
## [4,]   1 11 31  8 47  87.6
## [5,]   1  7 52  6 33  95.9
## [6,]   1 11 55  9 22 109.2&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Compute the &lt;strong&gt;augmented matrix of &lt;a href=&#34;https://en.wikipedia.org/wiki/Sufficient_statistic&#34;&gt;sufficient statistics&lt;/a&gt; &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; is the matrix having as elements the sum of the cross-products of the columns of &lt;code&gt;X_aug&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
T =
\begin{bmatrix}
n &amp;amp; \sum{x_1} &amp;amp; ... &amp;amp; \sum{x_p}\\
\sum{x_1} &amp;amp; \sum{x_1^2} &amp;amp; ... &amp;amp; \sum{x_1 x_p}\\
... &amp;amp; ... &amp;amp; ... &amp;amp; ...\\
\sum{x_p} &amp;amp; \sum{x_1 x_p} &amp;amp; ... &amp;amp; \sum{x_p^2}
\end{bmatrix}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Since the first column of &lt;code&gt;X_aug&lt;/code&gt; is a column of 1s, the first element of T is the number of rows in the data, the first column and rows store the sum of scores on each variable (sufficient statistics for the mean), and the other elements store the sum of products between the columns of &lt;code&gt;X&lt;/code&gt; (sufficient statistics for the covariance matrix of &lt;code&gt;X&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;In R, we can compute it easily with the cross-product function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Compute the matrix of sufficient statistics (T matrix)
  Tmat &amp;lt;- crossprod(X_aug)&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Transform T to G&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt; is simply &lt;span class=&#34;math inline&#34;&gt;\(T / n\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
G =
\begin{bmatrix}
 1 &amp;amp; \mu_1 &amp;amp; ... &amp;amp;\mu_p\\
 \mu_1 &amp;amp; \frac{\sum{x_1^2}}{n} &amp;amp; ... &amp;amp; \frac{\sum{x_1 x_p}}{n}\\
 ... &amp;amp; ... &amp;amp; ... &amp;amp; ...\\
 \mu_p &amp;amp; \frac{\sum{x_1 x_p}}{n} &amp;amp; ... &amp;amp; \frac{\sum{x_p^2}}{n}
\end{bmatrix}
\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Compute G
  G &amp;lt;- Tmat / n&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Compute &lt;span class=&#34;math inline&#34;&gt;\(\Theta\)&lt;/span&gt;&lt;/strong&gt; by sweeping G over the first row and column.&lt;/p&gt;
&lt;p&gt;Let’s use our &lt;code&gt;sweepGoodnight()&lt;/code&gt; function to perform SWEEP(1)[G] and obtain &lt;span class=&#34;math inline&#34;&gt;\(\Theta\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\Theta =
\begin{bmatrix}
1 &amp;amp; \mu_1 &amp;amp; ... &amp;amp;\mu_p\\
\mu_1 &amp;amp; \sigma^2_1 &amp;amp; ... &amp;amp; \sigma_{1p}\\
... &amp;amp; ... &amp;amp; ... &amp;amp; ...\\
\mu_p &amp;amp; \sigma_{1p} &amp;amp; ... &amp;amp; \sigma^2_{p}
\end{bmatrix}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In R:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Sweep G over the first position
  Theta &amp;lt;- sweepGoodnight(G, 1)

# Check how it looks
  Theta&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            int         V1         V2         V3          V4         V5
## int   1.000000   7.461538   48.15385  11.769231   30.000000   95.42308
## V1   -7.461538  31.940828   19.31361 -28.662722  -22.307692   59.68935
## V2  -48.153846  19.313609  223.51479 -12.810651 -233.923077  176.38107
## V3  -11.769231 -28.662722  -12.81065  37.869822    2.923077  -47.55621
## V4  -30.000000 -22.307692 -233.92308   2.923077  258.615385 -190.90000
## V5  -95.423077  59.689349  176.38107 -47.556213 -190.900000  208.90485&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Check Theta is storing the means in the first row and column
  colMeans(X)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        V1        V2        V3        V4        V5 
##  7.461538 48.153846 11.769231 30.000000 95.423077&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Check Theta is storing the ML covariance matrix everywhere else
  cov(X) * (n-1) / n&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           V1         V2         V3          V4         V5
## V1  31.94083   19.31361 -28.662722  -22.307692   59.68935
## V2  19.31361  223.51479 -12.810651 -233.923077  176.38107
## V3 -28.66272  -12.81065  37.869822    2.923077  -47.55621
## V4 -22.30769 -233.92308   2.923077  258.615385 -190.90000
## V5  59.68935  176.38107 -47.556213 -190.900000  208.90485&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Pay attention to a couple of things:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The covariance matrix stored in &lt;span class=&#34;math inline&#34;&gt;\(\Theta\)&lt;/span&gt; is the Maximum Likelihood version (denominator should be &lt;code&gt;n&lt;/code&gt; instead of the default &lt;code&gt;n-1&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;We could have constructed the object &lt;code&gt;Theta&lt;/code&gt; just by using &lt;code&gt;colMeans(X)&lt;/code&gt; and &lt;code&gt;cov(X) * (n-1) / n&lt;/code&gt; directly.
However, it is important to note the relationship between &lt;code&gt;Tmat&lt;/code&gt;, &lt;code&gt;G&lt;/code&gt;, and &lt;code&gt;Theta&lt;/code&gt;.
In particular, pay attention to the fact that &lt;code&gt;Theta&lt;/code&gt; is the result of sweeping &lt;code&gt;G&lt;/code&gt; in the first position.
When I started looking into this topic I did not understand this, and I kept sweeping &lt;code&gt;Theta&lt;/code&gt; over the first position, resulting in a confusing double sweeping of the first column and row.
I will get back to this point in a sec.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;estimate-multivariate-linear-models&#34; class=&#34;section level3&#34; number=&#34;2.2.2&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.2.2&lt;/span&gt; Estimate multivariate linear models&lt;/h3&gt;
&lt;p&gt;Now let’s see how we can use &lt;span class=&#34;math inline&#34;&gt;\(\Theta\)&lt;/span&gt; to estimate any multivariate linear model involving the variables in our dataset.
First, let’s see how we would obtain these linear models in R with standard procedures.
Say we want to regress V1 and V3 on V2, V4, and V5 from the &lt;code&gt;X&lt;/code&gt; dataset.
We will start by creating a formula for an &lt;code&gt;lm&lt;/code&gt; function to estimate the model we want.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Fit some multivariate linear models ------------------------------------------

  # Define the dependent variables (dvs) of the multivairate linear models
  dvs &amp;lt;- c(&amp;quot;V1&amp;quot;, &amp;quot;V3&amp;quot;)

  # Define the predictors (ivs) of the multivairate linear models
  ivs &amp;lt;- c(&amp;quot;V2&amp;quot;, &amp;quot;V4&amp;quot;, &amp;quot;V5&amp;quot;)

  # Create the formula (complicated but flexible way)
  formula_mlm &amp;lt;- paste0(&amp;quot;cbind(&amp;quot;,
                       paste0(dvs, collapse = &amp;quot;, &amp;quot;),
                       &amp;quot;) ~ &amp;quot;,
                       paste0(ivs, collapse = &amp;quot; + &amp;quot;))

  # Check the formula
  formula_mlm&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;cbind(V1, V3) ~ V2 + V4 + V5&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we will fit the multivariate linear model with the &lt;code&gt;lm()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  # Fit the model with the lm function
  mlm0 &amp;lt;- lm(formula_mlm, data = X)
  coef(mlm0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                      V1          V3
## (Intercept) -45.7660931 135.1150663
## V2           -0.2747666  -0.6559719
## V4            0.1455375  -1.0485195
## V5            0.6507081  -0.6319507&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These are our intercepts, and regression coefficients for the multivariate linear model.
We can sweep &lt;span class=&#34;math inline&#34;&gt;\(\Theta\)&lt;/span&gt; over the positions of the independent variables to obtain the the same intercept and regression coefficients.
First, let’s define a vector of positions to sweep over based on the variable names we stored in &lt;code&gt;ivs&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Fit some multivariate linear models using sweep ------------------------------

  # Define positions to sweep over
  sweep_over &amp;lt;- which(colnames(Theta) %in% ivs)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, let’s simply sweep our &lt;span class=&#34;math inline&#34;&gt;\(\Theta\)&lt;/span&gt; over these positions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  # Sweep theta
  H &amp;lt;- sweepGoodnight(Theta, K = sweep_over)

  # Check out the result
  H&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             int          V1           V2          V3           V4           V5
## int  612.422481 -45.7660931 -5.874822779 135.1150663 -6.469828251 -1.408803042
## V1    45.766093   1.6538239  0.274766592  -1.6628629 -0.145537538 -0.650708148
## V2    -5.874823  -0.2747666  0.085293622  -0.6559719  0.073716182 -0.004651691
## V3  -135.115066  -1.6628629  0.655971950   2.4781175  1.048519534  0.631950668
## V4    -6.469828   0.1455375  0.073716182  -1.0485195  0.075591156  0.006836668
## V5    -1.408803   0.6507081 -0.004651691  -0.6319507  0.006836668  0.014961788&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our regression coefficients are here in this new matrix.
We just need to find them.
We know that the dependent variables are V1 and V3, and that the independent variables are V2, V4, and V5.
Let’s index the rows of &lt;code&gt;H&lt;/code&gt; with the names of the ivs (and the name of the intercept row), and the columns of &lt;code&gt;H&lt;/code&gt; with the names of the dvs.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  # Extract the regression coefficients from H
  H[c(&amp;quot;int&amp;quot;, ivs), dvs]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              V1          V3
## int -45.7660931 135.1150663
## V2   -0.2747666  -0.6559719
## V4    0.1455375  -1.0485195
## V5    0.6507081  -0.6319507&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  # Compare with coefficients from lm function
  coef(mlm0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                      V1          V3
## (Intercept) -45.7660931 135.1150663
## V2           -0.2747666  -0.6559719
## V4            0.1455375  -1.0485195
## V5            0.6507081  -0.6319507&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that, we are sweeping &lt;span class=&#34;math inline&#34;&gt;\(\Theta\)&lt;/span&gt; only over the predictors, but we also get the estimate of the intercept.
Remember that &lt;span class=&#34;math inline&#34;&gt;\(\Theta\)&lt;/span&gt; is the result of sweeping G over the first position, which is the position where the intercept estimate appears.
You could obtain the same result by directly sweeping G over position 1, and the position of the predictors.
In code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  # Sweep G
  sweepGoodnight(G, c(1, sweep_over))[c(&amp;quot;int&amp;quot;, ivs), dvs]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              V1          V3
## int -45.7660931 135.1150663
## V2   -0.2747666  -0.6559719
## V4    0.1455375  -1.0485195
## V5    0.6507081  -0.6319507&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Therefore, you can think of finding the coefficients of a multivariate linear model using the sweep operator as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SWEEP(1, &lt;span class=&#34;math inline&#34;&gt;\(k_1, \dots, k_K\)&lt;/span&gt;)[G] or as,&lt;/li&gt;
&lt;li&gt;SWEEP(&lt;span class=&#34;math inline&#34;&gt;\(k_1, \dots, k_K\)&lt;/span&gt;)[SWEEP(1)[G]] or as,&lt;/li&gt;
&lt;li&gt;SWEEP(&lt;span class=&#34;math inline&#34;&gt;\(k_1, \dots, k_K\)&lt;/span&gt;)[&lt;span class=&#34;math inline&#34;&gt;\(\Theta\)&lt;/span&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;with &lt;span class=&#34;math inline&#34;&gt;\(k_1, \dots, k_K\)&lt;/span&gt; being the positions of the &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; predictors in matrix &lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Finally, just play around with what variables you consider as dvs and ivs.
You will discover the magic of the sweep operator.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Play around with variable roles ------------------------------------------

  # Define different dependent variables (dvs) for the multivairate linear models
  dvs &amp;lt;- c(&amp;quot;V1&amp;quot;, &amp;quot;V2&amp;quot;, &amp;quot;V5&amp;quot;)

  # Define different predictors (ivs) for the multivairate linear models
  ivs &amp;lt;- c(&amp;quot;V3&amp;quot;, &amp;quot;V4&amp;quot;)

  # Create the formula (complicated but flexible way)
  formula_mlm &amp;lt;- paste0(&amp;quot;cbind(&amp;quot;,
                       paste0(dvs, collapse = &amp;quot;, &amp;quot;),
                       &amp;quot;) ~ &amp;quot;,
                       paste0(ivs, collapse = &amp;quot; + &amp;quot;))

  # Fit the model with the MLM
  mlm1 &amp;lt;- lm(formula_mlm, data = X)
  coef(mlm1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                      V1         V2          V5
## (Intercept) 18.63186149 78.3607367 131.2824064
## V3          -0.75087203 -0.2686979  -1.1998512
## V4          -0.07777123 -0.9014841  -0.7246001&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  # Define positions to sweep over
  sweep_over &amp;lt;- which(colnames(Theta) %in% ivs)

  # Sweep Theta over new positions
  sweepGoodnight(Theta, K = sweep_over)[c(&amp;quot;int&amp;quot;, ivs), dvs]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              V1         V2          V5
## int 18.63186149 78.3607367 131.2824064
## V3  -0.75087203 -0.2686979  -1.1998512
## V4  -0.07777123 -0.9014841  -0.7246001&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;tldr-just-give-me-the-code&#34; class=&#34;section level1&#34; number=&#34;3&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; TL;DR, just give me the code!&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Write an R function implementing SWEEP(k)[G] according to Goodnight ----------

sweepGoodnight &amp;lt;- function (G, K){

  for(k in K){
    # Step 1: Let D = g_kk
    D &amp;lt;- G[k, k]

    # Step 2: Divide row k by D.
    G[k, ] &amp;lt;- G[k, ] / D

    # Step 3:
    # - For every other row i != k, let B = g_ik
    # - Subtract B \times row k from row i.
    # - set g_ik = -B/D.
    for(i in 1:nrow(G)){
      if(i != k){
        B &amp;lt;- G[i, k]
        G[i, ] &amp;lt;- G[i, ] - B * G[k, ]
        G[i, k] &amp;lt;- -1 * B / D
      }
    }
    # Step 4: Set g_kk = 1/D
    G[k, k] = 1/D
  }

  # Output
  return(G)
}

# Compare sweepGoodnight with other implementations ----------------------------

# Install the `fastmatrix` package (run if you don&amp;#39;t have it yet)
# install.packages(&amp;quot;fastmatrix&amp;quot;)

# Load fastmatrix
library(fastmatrix)

# Define an example dataset
X &amp;lt;- matrix(c(1, 1, 1, 1,
              1, 2, 1, 3,
              1, 3, 1, 3,
              1, 1,-1, 2,
              1, 2,-1, 2,
              1, 3,-1, 1), ncol = 4, byrow = TRUE)

# Define the G matrix
G &amp;lt;- crossprod(X)

# Define a vector of positions to sweep over
K &amp;lt;- 1:3

# Perform SWEEP[K](G) with fastmatrix sweep.operator
H_fm &amp;lt;- sweep.operator(G, k = K)

# Perform SWEEP[K](G) with our sweepGoodnight implementation
H_sg &amp;lt;- sweepGoodnight(G, K = K)

# Compare the two
all.equal(H_fm, H_sg)

# Load Little Rubin data -------------------------------------------------------

# Create data
  X &amp;lt;- as.data.frame(
          matrix(
                  data = c(7, 1, 11, 11, 7, 11, 3, 1, 2, 21, 1, 11, 10, 26,
                           29, 56, 31, 52, 55, 71 ,31, 54, 47, 40, 66, 68,
                           6, 15, 8, 8, 6, 9, 17, 22, 18, 4, 23, 9, 8,
                           60, 52, 20, 47, 33, 22,6,44,22,26,34,12,12,
                           78.5, 74.3, 104.3, 87.6, 95.9, 109.2, 102.7,
                           72.5, 93.1, 115.9, 83.8, 113.3, 109.4),
                  ncol = 5
          )
  )

# Store useful information
  n &amp;lt;- nrow(X)
  p &amp;lt;- ncol(X)

# Glance at the first 6 rows of the data
  head(X)

# Obtain the augmented covariance matrix ---------------------------------------

# Augment X
  X_aug &amp;lt;- cbind(int = 1, as.matrix(X))

# Glance at the first 6 rows of X_aug
  head(X_aug)

# Compute the matrix of sufficient statistics (T matrix)
  Tmat &amp;lt;- crossprod(X_aug)

# Compute G
  G &amp;lt;- Tmat / n

# Sweep G over the first position
  Theta &amp;lt;- sweepGoodnight(G, 1)

# Check how it looks
  Theta

# Check Theta is storing the means in the first row and column
  colMeans(X)

# Check Theta is storing the ML covariance matrix everywhere else
  cov(X) * (n-1) / n

# Fit some multivariate linear models ------------------------------------------

  # Define the dependent variables (dvs) of the multivairate linear models
  dvs &amp;lt;- c(&amp;quot;V1&amp;quot;, &amp;quot;V3&amp;quot;)

  # Define the predictors (ivs) of the multivairate linear models
  ivs &amp;lt;- c(&amp;quot;V2&amp;quot;, &amp;quot;V4&amp;quot;, &amp;quot;V5&amp;quot;)

  # Create the formula (complicated but flexible way)
  formula_mlm &amp;lt;- paste0(&amp;quot;cbind(&amp;quot;,
                       paste0(dvs, collapse = &amp;quot;, &amp;quot;),
                       &amp;quot;) ~ &amp;quot;,
                       paste0(ivs, collapse = &amp;quot; + &amp;quot;))

  # Check the formula
  formula_mlm

  # Fit the model with the lm function
  mlm0 &amp;lt;- lm(formula_mlm, data = X)
  coef(mlm0)

# Fit some multivariate linear models using sweep ------------------------------

  # Define positions to sweep over
  sweep_over &amp;lt;- which(colnames(Theta) %in% ivs)

  # Sweep theta
  H &amp;lt;- sweepGoodnight(Theta, K = sweep_over)

  # Check out the result
  H

  # Extract the regression coefficients from H
  H[c(&amp;quot;int&amp;quot;, ivs), dvs]

  # Compare with coefficients from lm function
  coef(mlm0)

  # Sweep G
  sweepGoodnight(G, c(1, sweep_over))[c(&amp;quot;int&amp;quot;, ivs), dvs]

# Play around with variable roles ------------------------------------------

  # Define different dependent variables (dvs) for the multivairate linear models
  dvs &amp;lt;- c(&amp;quot;V1&amp;quot;, &amp;quot;V2&amp;quot;, &amp;quot;V5&amp;quot;)

  # Define different predictors (ivs) for the multivairate linear models
  ivs &amp;lt;- c(&amp;quot;V3&amp;quot;, &amp;quot;V4&amp;quot;)

  # Create the formula (complicated but flexible way)
  formula_mlm &amp;lt;- paste0(&amp;quot;cbind(&amp;quot;,
                       paste0(dvs, collapse = &amp;quot;, &amp;quot;),
                       &amp;quot;) ~ &amp;quot;,
                       paste0(ivs, collapse = &amp;quot; + &amp;quot;))

  # Fit the model with the MLM
  mlm1 &amp;lt;- lm(formula_mlm, data = X)
  coef(mlm1)

  # Define positions to sweep over
  sweep_over &amp;lt;- which(colnames(Theta) %in% ivs)

  # Sweep Theta over new positions
  sweepGoodnight(Theta, K = sweep_over)[c(&amp;quot;int&amp;quot;, ivs), dvs]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-goodnight:1979&#34; class=&#34;csl-entry&#34;&gt;
Goodnight, James H. 1979. &lt;span&gt;“A Tutorial on the SWEEP Operator.”&lt;/span&gt; &lt;em&gt;The American Statistician&lt;/em&gt; 33 (3): 149–58.
&lt;/div&gt;
&lt;div id=&#34;ref-littleRubin:2002&#34; class=&#34;csl-entry&#34;&gt;
Little, R. J. A., and D. B. Rubin. 2002. &lt;em&gt;Statistical Analysis with Missing Data&lt;/em&gt;. 2nd ed. Hoboken, NJ: Wiley-Interscience.
&lt;/div&gt;
&lt;div id=&#34;ref-schafer:1997&#34; class=&#34;csl-entry&#34;&gt;
Schafer, Joseph L. 1997. &lt;em&gt;Analysis of Incomplete Multivariate Data&lt;/em&gt;. Vol. 72. Boca Raton, FL: Chapman &amp;amp; Hall/&lt;span&gt;CRC&lt;/span&gt;.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
