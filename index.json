[{"authors":null,"categories":null,"content":"I’m a PhD researcher at Tilburg University working on multiple imputation algorithms for survey data.\n  Download my resumé.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1659610082,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://edoardocostantini.github.io/author/edoardo-edo-costantini/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/edoardo-edo-costantini/","section":"authors","summary":"I’m a PhD researcher at Tilburg University working on multiple imputation algorithms for survey data.\n  Download my resumé.","tags":null,"title":"Edoardo (Edo) Costantini","type":"authors"},{"authors":["Edoardo (Edo) Costantini","Kyle M. Lang","Klaas Sijtsma","Tim Reeskens"],"categories":null,"content":"This a preliminary post meant to share the shiny app that can be used to scann the results. The article describing the results is coming soon. You may find the description of the simulation study in the readme file of the GitHub repository associated with this study.\nResults dashboard Thanks to this shiny app you can check all the simulation study conditions in early access.\n ","date":1661852460,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1659383554,"objectID":"83fd6c12f0b3e1a7882b9922a9a9ad16","permalink":"https://edoardocostantini.github.io/research/mi-super-pcr/","publishdate":"2022-08-01T09:41:00Z","relpermalink":"/research/mi-super-pcr/","section":"research","summary":"This is some summary.","tags":["missing values","imputation"],"title":"Supervised principal component regression imputation in MICE","type":"research"},{"authors":["Edoardo (Edo) Costantini"],"categories":["A primer on PCA","R notes"],"content":"  1 Introduction 2 R code notes 3 TL;DR, just give me the code! References   1 Introduction Principal covariates regression is a method to analyze the relationship between sets of multivariate data in the presence of highly-collinear variables. Compared to regular principal component regression, principal covariates regression PCovR extracts components that account for much of the variability in a set of \\(X\\) variables and that correlated well with a set of \\(Y\\) variables. For more information, I recommend reading Vervloet et al. (2015) and De Jong and Kiers (1992). In this post, you can find my R code notes on this method. In these notes, I show the computations used by the PCovR R-package to perform the method.\n 2 R code notes # Set up environment ----------------------------------------------------------- # Load pacakge that implements this method library(\u0026#34;PCovR\u0026#34;, verbose = FALSE, quietly = TRUE) # Load example data from PCovR package data(alexithymia) # Explore its scale colMeans(alexithymia$X) ## confused right words sensations describe ## 1.8196721 1.7950820 0.5983607 2.2213115 ## analyze problems upset puzzled let happen ## 2.5081967 1.6065574 1.1393443 1.2622951 ## identify essential feel about people describe more ## 1.6393443 2.7131148 1.7213115 1.0081967 ## going on why angry daily activities entertainment ## 0.9836066 1.2540984 1.6639344 1.6967213 ## reveal feelings close useful hidden meanings ## 1.6475410 2.8442623 2.4672131 1.2131148  colMeans(alexithymia$Y) ## CES-D RSE ## 16.46721 31.37295  apply(alexithymia$X, 2, var) ## confused right words sensations describe ## 1.388701 1.635348 1.151402 1.479542 ## analyze problems upset puzzled let happen ## 1.161089 1.678634 1.294472 1.302534 ## identify essential feel about people describe more ## 1.620919 1.231066 1.475410 1.363569 ## going on why angry daily activities entertainment ## 1.470803 1.496884 1.514226 1.601477 ## reveal feelings close useful hidden meanings ## 2.097886 1.240008 1.193131 1.045116  apply(alexithymia$Y, 2, var) ## CES-D RSE ## 118.35016 37.24199  # Subset data X_raw \u0026lt;- alexithymia$X y_raw \u0026lt;- alexithymia$Y[, 1, drop = FALSE] # Define paramters that can be useful n \u0026lt;- nrow(X_raw) p \u0026lt;- ncol(X_raw) # Scale data X \u0026lt;- scale(X_raw)# * (n - 1) / n y \u0026lt;- scale(y_raw)# * (n - 1) / n # Define parameters alpha \u0026lt;- .5 # weighting parameter npcs \u0026lt;- 5 # Estimation ------------------------------------------------------------------- # Estimate with PCovR function out \u0026lt;- PCovR::pcovr_est( X = X, Y = y, a = alpha, r = npcs # fixed number of components ) # Estimate manually (Vervolet version) Hx \u0026lt;- X %*% solve(t(X) %*% X) %*% t(X) G_vv \u0026lt;- alpha * X %*% t(X) / sum(X^2) + (1 - alpha) * Hx %*% y %*% t(y) %*% Hx / sum(y^2) EG_vv \u0026lt;- eigen(G_vv) # eigen-decomposition of matrix T_vv \u0026lt;- EG_vv$vectors[, 1:npcs] # Compare results -------------------------------------------------------------- # T scores Ts \u0026lt;- list( PCovR = head(out$Te), PCovR_man = head(X %*% out$W), Vervolet = head(T_vv) ) # Weights W \u0026lt;- list( PCovR = out$W, Vervolet = solve(t(X) %*% X) %*% t(X) %*% T_vv ) # Px ( t(out$Te) %*% X )[, 1:5] ## confused right words sensations describe analyze problems ## [1,] -8.4662585 -7.3375812 -2.2012950 6.1597661 0.4271093 ## [2,] 3.3642487 -2.5380829 -0.2684529 4.9595077 6.2443158 ## [3,] -1.4118650 -4.4348404 1.4484525 1.8944371 -3.1435113 ## [4,] 1.1136577 1.0030862 2.0443923 -3.5188316 0.5622115 ## [5,] 0.8172586 0.7913444 5.2341517 0.5130866 -5.2948505  ( t(out$W) %*% t(X) %*% X )[, 1:5] ## confused right words sensations describe analyze problems ## [1,] -8.4662585 -7.3375812 -2.2012950 6.1597661 0.4271093 ## [2,] 3.3642487 -2.5380829 -0.2684529 4.9595077 6.2443158 ## [3,] -1.4118650 -4.4348404 1.4484525 1.8944371 -3.1435113 ## [4,] 1.1136577 1.0030862 2.0443923 -3.5188316 0.5622115 ## [5,] 0.8172586 0.7913444 5.2341517 0.5130866 -5.2948505  out$Px[, 1:5] ## confused right words sensations describe analyze problems ## [1,] -8.4662585 -7.3375812 -2.2012950 6.1597661 0.4271093 ## [2,] 3.3642487 -2.5380829 -0.2684529 4.9595077 6.2443158 ## [3,] -1.4118650 -4.4348404 1.4484525 1.8944371 -3.1435113 ## [4,] 1.1136577 1.0030862 2.0443923 -3.5188316 0.5622115 ## [5,] 0.8172586 0.7913444 5.2341517 0.5130866 -5.2948505  # Py cbind( Py = out$Py, TtY = t(out$Te) %*% y, WtXtY = t(out$W) %*% t(X) %*% y ) ## CES-D CES-D CES-D ## [1,] -6.9315017 -6.9315017 -6.9315017 ## [2,] 0.7212995 0.7212995 0.7212995 ## [3,] 1.1114400 1.1114400 1.1114400 ## [4,] -0.2349069 -0.2349069 -0.2349069 ## [5,] -0.5126823 -0.5126823 -0.5126823  # B cbind( B = drop(out$B), WPY = drop(out$W %*% out$Py), WWtXtY = drop(out$W %*% t(out$W) %*% t(X) %*% y) ) ## B WPY WWtXtY ## [1,] 0.383110912 0.383110912 0.383110912 ## [2,] 0.019216602 0.019216602 0.019216602 ## [3,] -0.032288901 -0.032288901 -0.032288901 ## [4,] -0.016670641 -0.016670641 -0.016670641 ## [5,] 0.086442108 0.086442108 0.086442108 ## [6,] -0.185643598 -0.185643598 -0.185643598 ## [7,] 0.145228530 0.145228530 0.145228530 ## …","date":1659571200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1659609396,"objectID":"e1f927af65f2ab681f85ad5ea840d532","permalink":"https://edoardocostantini.github.io/post/pcovr/","publishdate":"2022-08-04T00:00:00Z","relpermalink":"/post/pcovr/","section":"post","summary":"An R script to understand how PCovR works.","tags":["PCA"],"title":"Principal covariates regression in R","type":"post"},{"authors":["Edoardo (Edo) Costantini","Kyle M. Lang","Klaas Sijtsma","Tim Reeskens"],"categories":null,"content":"Results dashboard Here you can explore the results reported in the simulation study. Thanks to this shiny app you can actually check all the additional results that couldn’t be included in the main text.\n","date":1656582060,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661940860,"objectID":"33132669c6b03686f7ce5b7dcf3dba32","permalink":"https://edoardocostantini.github.io/research/mipca-compare/","publishdate":"2022-07-30T09:41:00Z","relpermalink":"/research/mipca-compare/","section":"research","summary":"Multiple Imputation (MI) is one of the most popular approaches to addressing missing values in questionnaires and surveys. MI with multivariate imputation by chained equations (MICE) allows flexible imputation of many types of data. In MICE, for each variable under imputation, the imputer needs to specify which variables should act as predictors in the imputation model. The selection of these predictors is a difficult, but fundamental, step in the MI procedure, especially when there are many variables in a data set. In this project, we explore the use of principal component regression (PCR) as a univariate imputation method in the MICE algorithm to automatically address the \"many variables\" problem that arises when imputing large social science data. We compare different implementations of PCR-based MICE with a correlation-thresholding strategy by means of a Monte Carlo simulation study and a case study. We find the use of PCR on a variable-by-variable basis to perform best and that it can perform closely to expertly designed imputation procedures.","tags":["missing values","imputation"],"title":"Solving the \"many variables\" problem in MICE with principal component regression","type":"research"},{"authors":["Edoardo (Edo) Costantini"],"categories":["Knowledge snippet"],"content":"   Introduction The residual standard error is a measure of fit for linear regression models. Conceptually, it can be thought of as the variability of the prediction error for a linear model. It is usually calculated as:\n\\[ SE_{resid} = \\sqrt{ \\frac{ \\sum^{n}_{i = 1}(y_i - \\hat{y}_i)^2 }{df_{resid}} } \\]\nwhere:\n \\(n\\) is the sample size \\(k\\) is the number of parameters to estimate in the model \\(-1\\) is the degree of freedom lost to estimate the intercept \\(\\hat{y}_i\\) is the fitted \\(y\\) value for the \\(i\\)-th individual \\(df_{resid}\\) is the degrees of freedom of the residuals (\\(n - k - 1\\))  The smaller the residual standard error, the better the model fits the data.\n Learn by coding We can compute the residual standard error manually after estimating a linear model in R. To get a better grasp of the residual standard error, let’s start by regressing the miles per gallon (mpg) on the number of cylinders (cyl), horsepower (hp), and weight (wt) of cars from the standard mtcars R dataset.\n# Fit a linear model ----------------------------------------------------------- lm_fit \u0026lt;- lm(mpg ~ cyl + hp + wt, data = mtcars) We can compute the residual standard error following the formula described above:\n# Compute the residual standard error manually --------------------------------- # Define elements of the formula n \u0026lt;- nrow(mtcars) # sample size k \u0026lt;- 3 # number of parameters (regression coefficients) yhat \u0026lt;- fitted(lm_fit) # fitted y values y \u0026lt;- mtcars$mpg # Compute rse rse \u0026lt;- sqrt(sum((y - yhat)^2) / (n - k - 1)) # Print rse rse ## [1] 2.511548 We can also extract it directly from any lm object:\n# residual standard error from lm output --------------------------------------- # Use the sigma function to extract it from an lm object sigma(lm_fit) ## [1] 2.511548  # Compare with the manual computation sigma(lm_fit) - rse ## [1] 0  TL;DR, just give me the code! # Fit a linear model ----------------------------------------------------------- lm_fit \u0026lt;- lm(mpg ~ cyl + hp + wt, data = mtcars) # Compute the residual standard error manually --------------------------------- # Define elements of the formula n \u0026lt;- nrow(mtcars) # sample size k \u0026lt;- 3 # number of parameters (regression coefficients) yhat \u0026lt;- fitted(lm_fit) # fitted y values y \u0026lt;- mtcars$mpg # Compute rse rse \u0026lt;- sqrt(sum((y - yhat)^2) / (n - k - 1)) # Print rse rse # residual standard error from lm output --------------------------------------- # Use the sigma function to extract it from an lm object sigma(lm_fit) # Compare with the manual computation sigma(lm_fit) - rse  Other resources  Statology: How to Interpret Residual Standard Error Statology: How to Calculate Residual Standard Error in R   ","date":1655856000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656054930,"objectID":"e628131a7334417662e8dc0b2a5a090c","permalink":"https://edoardocostantini.github.io/post/residual-standard-error/","publishdate":"2022-06-22T00:00:00Z","relpermalink":"/post/residual-standard-error/","section":"post","summary":"Introduction The residual standard error is a measure of fit for linear regression models. Conceptually, it can be thought of as the variability of the prediction error for a linear model.","tags":["linear models","ols"],"title":"Understanding the residual standard error","type":"post"},{"authors":["Edoardo (Edo) Costantini","Kyle M. Lang","Klaas Sijtsma","Tim Reeskens"],"categories":null,"content":"Results dashboard Here you can explore the results reported in the simulation study. Thanks to this shiny app you can check all the additional results that couldn’t be included in the main text.\nCOMING SOON\n --","date":1654076460,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661940843,"objectID":"dc0b4233bcf700a6c51552d30371ded6","permalink":"https://edoardocostantini.github.io/research/hd-compare/","publishdate":"2022-06-01T09:41:00Z","relpermalink":"/research/hd-compare/","section":"research","summary":"Including a large number of predictors in the imputation model underlying a multiple imputation (MI) procedure is one of the most challenging tasks imputers face. A variety of high-dimensional MI techniques can help, but there has been limited research on their relative performance. In this study, we investigated a wide range of extant high-dimensional MI techniques that can handle a large number of predictors in the imputation models and general missing data patterns. We assessed the relative performance of seven high-dimensional MI methods with a Monte Carlo simulation study and a resampling study based on real survey data. The performance of the methods was deﬁned by the degree to which they facilitate unbiased and conﬁdence-valid estimates of the parameters of complete data analysis models. We found that using regularized regression to select the predictors used in the MI model and using principal component analysis to reduce the dimensionality of auxiliary data produce the best results.","tags":["missing values","imputation","machine learning","high-dimensionality"],"title":"High-dimensional imputation for the social sciences: a comparison of state-of-the-art methods","type":"research"},{"authors":[],"categories":["Machine Learning"],"content":"   Introduction Cross-entropy (CE) quantifies the difference between two probability distributions. As such, it comes in handy as a loss function in multi-class classification tasks (e.g., multinomial logistic regression). It also provides an elegant solution for determining the difference between actual and predicted categorical data point values. It can be used to determine the predictive performance of a classification model. The value of the cross-entropy is higher when the predicted classes diverge more from the true labels.\n Learn by coding In a multiclass-classification task, we calculate a separate “loss” for each class for each observation and sum the result:\n\\[ CE = - \\sum^{N}_{i = 1} \\sum^{K}_{k = 1} p_{(i, k)}log(\\hat{p}_{(i, k)}) \\tag{1} \\]\nwhere\n \\(N\\) is the sample size. \\(K\\) is the number of categories of the variable we are trying to predict. \\(p\\) is a scalar taking value \\(0 = \\text{no}\\) or \\(1 = \\text{yes}\\) to indicate whether observation \\(i\\) belongs to class \\(k\\). This can also be thought of as the true probability of the observation belonging to that class. \\(\\hat{p}\\) is a scalar indicating the predicted probability of observation \\(i\\) belonging to class \\(k\\). \\(log\\) is the natural logarithm.  Let’s see an example in R. The iris data records the petal and sepal dimensions for 150 and their species. Consider the task of predicting the flowers’ species based on all the numeric predictors available. We will fit a multinomial logistic regression on the data and compute the cross-entropy between the observed and predicted class membership.\nTo start, we should prepare the R environment by loading a few packages we will use:\n nnet to estimate the multinomial logistic model; MLmetric to check someone else’s implementation of the cross-entropy computation. FactoMineR to create a disjunctive table from an R factor  # Prepare environment ---------------------------------------------------------- # Packages library(nnet) library(MLmetrics) # for LogLoss() function library(FactoMineR) # for tab.disjonctif() function # Default rounding for this sessino options(\u0026#34;digits\u0026#34; = 5) Then, we should estimate the multinomial logistic model of interest. We will use this model to create predictions.\n# Fit mulinomial logistic model ------------------------------------------------ # Fit model glm_mln \u0026lt;- multinom(Species ~ Sepal.Length, data = iris) We can now create two R matrices p and p_hat storing all the scalars \\(p_{ik}\\) and \\(\\hat{p}_{ik}\\) we need to compute (1).\n First, we want to store all the \\(p_{ik}\\) in one matrix. To do so, we can create a disjunctive table based on the species factor. This is an \\(N \\times K\\) matrix storing 0s and 1s to indicate which class every observation belongs to.\n# Obtain p and p_har ----------------------------------------------------------- # store true labels in a matrix p p \u0026lt;- FactoMineR::tab.disjonctif(iris$Species) # check it head(p) ## setosa versicolor virginica ## 1 1 0 0 ## 2 1 0 0 ## 3 1 0 0 ## 4 1 0 0 ## 5 1 0 0 ## 6 1 0 0 Second, we want to obtain the predicted class probabilities for every observation:\n# obtain predictions p_hat \u0026lt;- predict(glm_mln, type = \u0026#34;probs\u0026#34;) # check it head(p_hat) ## setosa versicolor virginica ## 1 0.80657 0.176155 0.0172792 ## 2 0.91844 0.076558 0.0050018 ## 3 0.96787 0.030792 0.0013399 ## 4 0.98005 0.019262 0.0006841 ## 5 0.87281 0.117765 0.0094276 ## 6 0.47769 0.442466 0.0798435  We can now write a loop to perform the computation in (1) for every \\(i\\) and \\(k\\).\n# Compute CE with a loop ------------------------------------------------------- # Define parameters N \u0026lt;- nrow(iris) # sample size K \u0026lt;- nlevels(iris$Species) # number of classes # Create storing object for CE CE \u0026lt;- 0 # Compute CE with a loop for (i in 1:N){ for (k in 1:K){ CE \u0026lt;- CE - p[i, k] * log(p_hat[i, k]) } } # Print the value of CE CE ## [1] 91.034 We can also work with the matrices p and p_hat directly to avoid using a loop:\n# Compute CE using the matrices directly --------------------------------------- ce \u0026lt;- -sum(diag(p %*% t(log(p_hat)))) # Print the value of ce ce ## [1] 91.034 This approach works for a binary prediction just as well. We only need to pay attention to storing the true and predicted probabilities in matrix form. For example, consider the task of predicting the transmission type (automatic or not) for the cars recorded in the mtcars dataset.\n# Binary cross entropy --------------------------------------------------------- # Fit model glm_log \u0026lt;- glm(am ~ hp + wt, family = binomial(link = \u0026#39;logit\u0026#39;), data = mtcars) # store true labels in a matrix p p \u0026lt;- FactoMineR::tab.disjonctif(mtcars$am) # obtain predicted probabilites in matrix form pred_probs \u0026lt;- predict(glm_log, type = \u0026#34;response\u0026#34;) p_hat \u0026lt;- cbind(k_0 = 1 - pred_probs, k_1 = pred_probs) The objects p and p_hat are all the information we need to compute the cross-entropy for this binary prediction task:\n# check the first few rows of p head(p) ## [,1] [,2] ## 1 0 1 ## 2 0 1 ## 3 0 1 ## 4 1 0 ## …","date":1650585600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652780301,"objectID":"dff6d4f0d8712b8c1607d3618073920e","permalink":"https://edoardocostantini.github.io/post/cross-entropy/","publishdate":"2022-04-22T00:00:00Z","relpermalink":"/post/cross-entropy/","section":"post","summary":"Introduction Cross-entropy (CE) quantifies the difference between two probability distributions. As such, it comes in handy as a loss function in multi-class classification tasks (e.g., multinomial logistic regression).","tags":["prediction","outcome measures"],"title":"Cross-entropy as a measure of predictive performance","type":"post"},{"authors":["Edoardo (Edo) Costantini"],"categories":["Drafts","Tutorials"],"content":"   1 Introduction 2 Learn by coding  2.1 Example 2.2 Computing the weighted covariance matrix manually  2.2.1 Exploring the stats::cov.wt() function code 2.2.2 Reproducing the internal steps  2.3 Mathematical formula and alternative R computations  2.3.1 Unbiased weighted covariance matrix 2.3.2 Maximum Likelihood weighted covariance matrix  2.4 Relationship with the matrix of sufficient statistics  3 TL;DR, just give me the code!   1 Introduction In a sample made of groups of different sizes, descriptive statistics like the mean and the covariance between variables can be computed by assigning proper weights to account for the difference in group sizes. Wights are generally normalized (i.e., \\(\\sum_{i = 1}^{n} w_i = 1\\)).\n 2 Learn by coding 2.1 Example Now, let’s consider a very simple example. Say that you have a dataset with two variables and that you have a vector of weights defining how important each observation should be.\n# Initial simple example ------------------------------------------------------- # Get the dataset used in the example of stats::cov.wt() xy \u0026lt;- cbind(x = 1:10, y = c(1:3, 8:5, 8:10)) # Define non-negative weights (as in example of stats::cov.wt()) wi \u0026lt;- c(0,0,0,1,1,1,1,1,0,0) # Get the weighted estimate with the default methods covwt_stats \u0026lt;- stats::cov.wt(xy, wt = wi) # i.e. method = \u0026#34;unbiased\u0026#34; # Compare unweighted and weighted means data.frame(uw = colMeans(xy), select = colMeans(xy[wi == 1, ]), wg = covwt_stats$center) ## uw select wg ## x 5.5 6.0 6.0 ## y 5.9 6.8 6.8  # Compare unweighted and weighted covariance matrix data.frame(uw = c(cov(xy)), select = c(cov(xy[wi == 1, ])), wg = c(covwt_stats$cov), row.names = c(sapply(colnames(cov(xy)), paste0, rownames(cov(xy)))) ) ## uw select wg ## xx 9.166667 2.5 2.5 ## xy 8.055556 -0.5 -0.5 ## yx 8.055556 -0.5 -0.5 ## yy 9.433333 1.7 1.7 Note how by weighting with a vector of 0 and 1s we are basically saying that the observations with a 0 will be excluded from the count. They are weighted to have 0 impact on the computation of the descriptive statistics. This is clear when you compare the results of the select and wg columns.\n 2.2 Computing the weighted covariance matrix manually We could replicate the results of the weighting simply by selecting a subset of the original data because all observations were either weighted 0 or equally (1). When this is not the case, weighting is slightly more complicated.\n2.2.1 Exploring the stats::cov.wt() function code Let’s look at how the cov.wt() function works more in depth. The internal code of the function is the following:\n# Examine the internal code of stats::cov.wt() --------------------------------- cov.wt ## function (x, wt = rep(1/nrow(x), nrow(x)), cor = FALSE, center = TRUE, ## method = c(\u0026#34;unbiased\u0026#34;, \u0026#34;ML\u0026#34;)) ## { ## if (is.data.frame(x)) ## x \u0026lt;- as.matrix(x) ## else if (!is.matrix(x)) ## stop(\u0026#34;\u0026#39;x\u0026#39; must be a matrix or a data frame\u0026#34;) ## if (!all(is.finite(x))) ## stop(\u0026#34;\u0026#39;x\u0026#39; must contain finite values only\u0026#34;) ## n \u0026lt;- nrow(x) ## if (with.wt \u0026lt;- !missing(wt)) { ## if (length(wt) != n) ## stop(\u0026#34;length of \u0026#39;wt\u0026#39; must equal the number of rows in \u0026#39;x\u0026#39;\u0026#34;) ## if (any(wt \u0026lt; 0) || (s \u0026lt;- sum(wt)) == 0) ## stop(\u0026#34;weights must be non-negative and not all zero\u0026#34;) ## wt \u0026lt;- wt/s ## } ## if (is.logical(center)) { ## center \u0026lt;- if (center) ## colSums(wt * x) ## else 0 ## } ## else { ## if (length(center) != ncol(x)) ## stop(\u0026#34;length of \u0026#39;center\u0026#39; must equal the number of columns in \u0026#39;x\u0026#39;\u0026#34;) ## } ## x \u0026lt;- sqrt(wt) * sweep(x, 2, center, check.margin = FALSE) ## cov \u0026lt;- switch(match.arg(method), unbiased = crossprod(x)/(1 - ## sum(wt^2)), ML = crossprod(x)) ## y \u0026lt;- list(cov = cov, center = center, n.obs = n) ## if (with.wt) ## y$wt \u0026lt;- wt ## if (cor) { ## Is \u0026lt;- 1/sqrt(diag(cov)) ## R \u0026lt;- cov ## R[] \u0026lt;- Is * cov * rep(Is, each = nrow(cov)) ## y$cor \u0026lt;- R ## } ## y ## } ## \u0026lt;bytecode: 0x7f8135ac48d0\u0026gt; ## \u0026lt;environment: namespace:stats\u0026gt; Note the following:\n The first thing to pay attention to is that the function can compute the weighted covariance matrix in two ways:\n unbiased, using corssprod(x) / (1 - sum(wt^2)) ML (or maximum likelihood), using corssprod(x)  Note that the wt object is divided by the sum of the values it is storing, which amounts to normalising the weights. This happens with wt \u0026lt;- wt/s with s being created inside an if statement as s \u0026lt;- sum(wt).\n x is centered on the normalized weigthed means using the sweep function\n x is weighted by multiplying by sqrt(wt)\n   2.2.2 Reproducing the internal steps First, we’ll set up a few objects we need to replicate manually what happens inside the stats::cov.wt() function. We need to define a dataset, a vector of weights, a method to compute descriptives, and based on these we will also create an object to store the number of rows (n). As a vector of weights we sample random values between 0 and 1. We can think of this as an attempt to weight each observation for the probability of sampling them from a population.\n# Set up manual computation of cov.wt() …","date":1647216000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650753679,"objectID":"dfd82daeec56c4cfec7556dd9d2e1ebb","permalink":"https://edoardocostantini.github.io/post/covmatwt/","publishdate":"2022-03-14T00:00:00Z","relpermalink":"/post/covmatwt/","section":"post","summary":"1 Introduction 2 Learn by coding  2.1 Example 2.2 Computing the weighted covariance matrix manually  2.2.1 Exploring the stats::cov.wt() function code 2.2.2 Reproducing the internal steps  2.","tags":["statistics","weights","covariance"],"title":"Estimating the weighted covariance matrix in R","type":"post"},{"authors":["Edoardo (Edo) Costantini"],"categories":["Drafts","Tutorials"],"content":"   1 Introduction 2 Learn by coding  2.1 Fitting ridge regression manually  2.1.1 An alternative way to avoid penalising the intercept  2.2 Fit ridge regression with glmnet  2.2.1 Use the biased estimation of variance 2.2.2 Return the unstandardized coefficients 2.2.3 Adjust the parametrization of \\(\\lambda\\) for glmnet 2.2.4 Compare manual and glmnet ridge regression output   3 TL;DR, just give me the code!   1 Introduction When there are many correlated predictors in a linear regression model, their regression coefficients can become poorly determined and exhibit high variance. This problem can be alleviated by imposing a size constraint (or penalty) on the coefficients. Ridge regression shrinks the regression coefficients by imposing a penalty on their size. The ridge coefficients values minimize a penalized residual sum of squares:\n\\[ \\hat{\\beta}^{\\text{ridge}} = \\text{argmin}_{\\beta} \\left\\{ \\sum_{i=1}^{N} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p} x_{ij}\\beta_j \\right)^2 + \\lambda \\sum_{j=1}^{p}\\beta_j^2 \\right\\} \\]\nThe ridge solutions are not equivariant under scaling of the inputs. Therefore, it is recommended to standardize the inputs before solving the minimization problem.\nNotice that the intercept \\(\\beta_0\\) has been left out of the penalty term. Penalization of the intercept would make the procedure depend on the origin chosen for \\(Y\\). Furthermore, by centering the predictors, we can separate the solution to the minimazion problem into two parts:\nIntercept \\[ \\hat{\\beta}_0 = \\bar{y}=\\frac{1}{N}\\sum_{i = 1}^{N} y_i \\]\n Penalised regression coefficients \\[ \\hat{\\beta}^{\\text{ridge}}=(\\mathbf{X}^T\\mathbf{X} + \\lambda \\mathbf{I})^{-1}\\mathbf{X}^Ty \\] which is the regular way of estimating regression coefficients with a penalty term (\\(\\lambda\\)) added on the diagonal (\\(\\mathbf{I}\\)) of the cross-product matrix (\\(\\mathbf{X}^T\\mathbf{X}\\)) to make it invertible (\\((...)^{-1}\\)).\n   2 Learn by coding The glmnet package can be used to obtain the ridge regression estimates of the regression coefficients. In this section, we will first see how to obtain these estimates “manually”, that is coding every step on our own, and then we will see how to obtain the same results using the glmnet package.\nLet’s start by setting up the R environment. In this post, we will work with the mtcars data. If you are not familiar with it, just look up the R help file on it. We will use the first column of the dataset (variable named mpg) as a dependent variable and the remaining ones as predictors in a linear regression.\n# Set up ----------------------------------------------------------------------- # Load packages library(glmnet) # Take the mtcars data y \u0026lt;- mtcars[, \u0026#34;mpg\u0026#34;] X \u0026lt;- mtcars[, -1] # Create a few shorthands we will use n \u0026lt;- nrow(X) p \u0026lt;- ncol(X) 2.1 Fitting ridge regression manually First, let’s make sure the predictors are centered on the mean and scaled to have a variance of 1.\n# Fitting ridge regression manually -------------------------------------------- # Scale the data (standardize) X_scale \u0026lt;- scale(X, center = TRUE, scale = TRUE) Then, we want to fit the ridge regression manually by separating the intercept and the regression coefficients estimation (two-step approach):\nEstimate the intercept (\\(\\hat{\\beta}_0\\))\n# Estimate the intercept b0_hat_r \u0026lt;- mean(y) Estimate the ridge regression coefficients (\\(\\hat{\\beta}^{\\text{ridge}}\\)).\n  Compute the cross-product matrix of the predictors.\nThis is the same step we would take if we wanted to compute the OLS estimates.\n# Compute the cross-product matrix of the data XtX \u0026lt;- t(X_scale) %*% X_scale Define a value of \\(\\lambda\\).\nThis value is usually chosen by cross-validation from a grid of possible values. However, here we are only interested in how \\(\\lambda\\) is used in the computation, so we can simply give it a fixed value.\n# Define a lambda value lambda \u0026lt;- .1 Compute \\(\\hat{\\beta}^{\\text{ridge}}\\).\n# Estimate the regression coefficients with the ridge penalty bs_hat_r \u0026lt;- solve(XtX + lambda * diag(p)) %*% t(X_scale) %*% y where diag(p) is the identity matrix \\(\\mathbf{I}\\).\n  Finally, let’s print the results:\n# Print the results round( data.frame(twostep = c(b0 = b0_hat_r, b = bs_hat_r)), 3 ) ## twostep ## b0 20.091 ## b1 -0.194 ## b2 1.366 ## b3 -1.373 ## b4 0.438 ## b5 -3.389 ## b6 1.361 ## b7 0.162 ## b8 1.243 ## b9 0.496 ## b10 -0.460 It is important to note the effect of centering and scaling. When fitting ridge regression, many sources recommend centering the data. This allows separating the estimation of the intercept from the estimation of the regression coefficients. As a result, only the regression coefficients are penalised. To understand the effect of centering, consider what happens in regular OLS estimation when predictors are centered:\n# Centering in regular OLS ----------------------------------------------------- # Create a version of X that is centered X_center \u0026lt;- scale(X, center = TRUE, scale = FALSE) # Fit an regular linear model lm_ols \u0026lt;- …","date":1646006400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653329109,"objectID":"002b3ab0e0f6e5e68210162d4766a05b","permalink":"https://edoardocostantini.github.io/post/ridge/","publishdate":"2022-02-28T00:00:00Z","relpermalink":"/post/ridge/","section":"post","summary":"1 Introduction 2 Learn by coding  2.1 Fitting ridge regression manually  2.1.1 An alternative way to avoid penalising the intercept  2.2 Fit ridge regression with glmnet  2.","tags":["statistics","regression","penalty","high-dimensional data"],"title":"Estimating ridge regression in R","type":"post"},{"authors":["Edoardo (Edo) Costantini"],"categories":["The EM algorithm"],"content":"   1 Introduction 2 Learn by coding  2.1 Coding a sweep function in R 2.2 Using the sweep operator to estimate regression models  2.2.1 Compute the augmented covariance matrix 2.2.2 Estimate multivariate linear models   3 TL;DR, just give me the code! References   1 Introduction The sweep operator is a matrix transformation commonly used to estimate regression models. It performs elementary row operations on a \\(p \\times p\\) matrix which happen to be particularly useful to estimate multivariate linear models. Little and Rubin (2002, p148) defined it as follows:\n The sweep operator is defined for symmetric matrices as follows. A \\(p \\times p\\) symmetric matrix G is said to be swept on row and column k if it is replaced by another symmetric \\(p \\times p\\) matrix H with elements defined as follows: \\[ h_{kk} = -1/g_{kk} \\] \\[ h_{jk} = h_{kj} = \\frac{g_{jk}}{g_{kk}}, j \\neq k \\] \\[ h_{jl} = g_{jl} - \\frac{g_{jk}g_{kl}}{g_{kk}}, j \\neq k, l \\neq k \\]\n The notation indicating this transformation is usually a variation of \\(\\text{SWEEP}[k]G\\), which can be read as sweeping matrix \\(G\\) on column (and row) \\(k\\). It is important to know that:\n Any symmetric matrix \\(G\\) can be swept over \\(l\\) multiple positions. The notation \\(\\text{SWEEP}[k_1, k_2, ..., k_l]G\\) indicates successive applications of \\(\\text{SWEEP}[k]G\\) with \\(k = k_1, \\dots, k_l\\). The sweep operator is commutative. Sweeps on multiple positions do not need to be carried out in any particular order:  \\[ \\text{SWEEP}[k_2]\\text{SWEEP}[k_1]G = \\text{SWEEP}[k_1]\\text{SWEEP}[k_2]G \\]\n The \\(l\\) sweeping positions do not need to be consecutive. For example, \\(k_1\\) could indicate the third column and \\(k_2\\) could indicate the sixth column.  In this post, I want to show how the sweep operator can be used to estimate the parameters of any linear regressions model. If you are interested in the mathematical details, I recommend reading the full sweep operator description in Goodnight (1979, p154), Schafer (1997), or Little and Rubin (2002, p148).\nGoodnight (1979, p150) is a particularly helpful paper as it describes an easy implementation of the sweep operator. Following Goodnight, given an originally symmetric positive definite matrix G, \\(\\text{SWEEP}[k]G\\) modifies a matrix G as follows:\n Step 1: Let \\(D = g_{kk}\\) Step 2: Divide row \\(k\\) by \\(D\\). Step 3: For every other row \\(i \\neq k\\), let \\(B = g_{ik}\\). Subtract \\(B \\times \\text{row } k\\) from row \\(i\\). Set \\(g_{ik} = -B/D\\). Step 4: Set \\(g_{kk} = 1/D\\).   2 Learn by coding 2.1 Coding a sweep function in R Let’s start by coding a simple function that performs the operations described by Goodnight (1979, p150). We want a function that takes as inputs a symmetric matrix (argument G) and a vector of positions to sweep over (argument K). The function below takes these two inputs and performs the four sweep steps for every element of K.\n# Write an R function implementing SWEEP(k)[G] according to Goodnight ---------- sweepGoodnight \u0026lt;- function (G, K){ for(k in K){ # Step 1: Let D = g_kk D \u0026lt;- G[k, k] # Step 2: Divide row k by D. G[k, ] \u0026lt;- G[k, ] / D # Step 3: # - For every other row i != k, let B = g_ik # - Subtract B \\times row k from row i. # - set g_ik = -B/D. for(i in 1:nrow(G)){ if(i != k){ B \u0026lt;- G[i, k] G[i, ] \u0026lt;- G[i, ] - B * G[k, ] G[i, k] \u0026lt;- -1 * B / D } } # Step 4: Set g_kk = 1/D G[k, k] = 1/D } # Output return(G) } Let’s check that this function returns what we want by comparing it with a function implemented by someone else.\n# Compare sweepGoodnight with other implementations ---------------------------- # Install the `fastmatrix` package (run if you don\u0026#39;t have it yet) # install.packages(\u0026#34;fastmatrix\u0026#34;) # Load fastmatrix library(fastmatrix) # Define an example dataset X \u0026lt;- matrix(c(1, 1, 1, 1, 1, 2, 1, 3, 1, 3, 1, 3, 1, 1,-1, 2, 1, 2,-1, 2, 1, 3,-1, 1), ncol = 4, byrow = TRUE) # Define the G matrix G \u0026lt;- crossprod(X) # Define a vector of positions to sweep over K \u0026lt;- 1:3 # Perform SWEEP[K]G with fastmatrix sweep.operator H_fm \u0026lt;- sweep.operator(G, k = K) # Perform SWEEP[K]G with our sweepGoodnight implementation H_sg \u0026lt;- sweepGoodnight(G, K = K) # Compare the two all.equal(H_fm, H_sg) ## [1] TRUE The functions fastmatrix::sweep.operator() and sweepGoodnight() return the same H matrix by sweeping matrix G over the positions defined in K.\n 2.2 Using the sweep operator to estimate regression models To understand how the sweep operator relates to the estimation of multivariate linear models, we will work with a data set used by Little and Rubin (2002, p152).\n# Load Little Rubin data ------------------------------------------------------- # Create data X \u0026lt;- as.data.frame( matrix( data = c(7, 1, 11, 11, 7, 11, 3, 1, 2, 21, 1, 11, 10, 26, 29, 56, 31, 52, 55, 71 ,31, 54, 47, 40, 66, 68, 6, 15, 8, 8, 6, 9, 17, 22, 18, 4, 23, 9, 8, 60, 52, 20, 47, 33, 22,6,44,22,26,34,12,12, 78.5, 74.3, 104.3, 87.6, 95.9, 109.2, 102.7, 72.5, 93.1, 115.9, 83.8, 113.3, 109.4), ncol = 5 ) ) # Store useful information n \u0026lt;- …","date":1637107200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652973141,"objectID":"e7ade286a1e858aadfd2e69770bb61ac","permalink":"https://edoardocostantini.github.io/post/sweep/","publishdate":"2021-11-17T00:00:00Z","relpermalink":"/post/sweep/","section":"post","summary":"Series: The EM algorithm - Part 1","tags":["statistics","regression"],"title":"The sweep operator","type":"post"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649712015,"objectID":"8576ec274c98b3831668a172fa632d80","permalink":"https://edoardocostantini.github.io/about/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/about/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648380937,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://edoardocostantini.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]