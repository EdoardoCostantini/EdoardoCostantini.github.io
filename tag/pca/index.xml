<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>PCA | Edo</title>
    <link>https://edoardocostantini.github.io/tag/pca/</link>
      <atom:link href="https://edoardocostantini.github.io/tag/pca/index.xml" rel="self" type="application/rss+xml" />
    <description>PCA</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 16 May 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://edoardocostantini.github.io/media/icon_hud9fa8ac6cd4a8ee42e0d32197c4c8f6f_199053_512x512_fill_lanczos_center_3.png</url>
      <title>PCA</title>
      <link>https://edoardocostantini.github.io/tag/pca/</link>
    </image>
    
    <item>
      <title>Deciding the Number of PCs with Non-Graphical Solutions to the Scree Test</title>
      <link>https://edoardocostantini.github.io/post/pca-non-graphical-solutions/</link>
      <pubDate>Mon, 16 May 2022 00:00:00 +0000</pubDate>
      <guid>https://edoardocostantini.github.io/post/pca-non-graphical-solutions/</guid>
      <description>
&lt;script src=&#34;https://edoardocostantini.github.io/post/pca-non-graphical-solutions/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;

&lt;/div&gt;

&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Different solutions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Kaiser Rule (aka Optimal Coordinate) &lt;span class=&#34;math inline&#34;&gt;\(n_{oc}\)&lt;/span&gt;.
In its simplest form, the Kaiserâ€™s rule retains only the PCs with variances exceeding 1.
If a PC has less variance than 1, it means that it explains less total variance than a single variable in the data, which makes it useless.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Acceleration Factor.
For every &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th eigenvalue, the acceleration factor &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; is calculated as the change in the slope between the line connecting the &lt;span class=&#34;math inline&#34;&gt;\(eig_j\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(eig_{j-1}\)&lt;/span&gt;, and the line connecting &lt;span class=&#34;math inline&#34;&gt;\(eig_j\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(eig_{j+1}\)&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[
a_{j} = (eig_{j+1} - eig_{j}) - (eig_{j} - eig_{j-1})
\]&lt;/span&gt;
Once the largest &lt;span class=&#34;math inline&#34;&gt;\(a_j\)&lt;/span&gt; is found, the number of components is set to &lt;span class=&#34;math inline&#34;&gt;\(j-1\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;learn-by-coding&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Learn by coding&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Prepare environment ----------------------------------------------------------

library(nFactors)
library(psych)

# Perform PCA
res &amp;lt;- psych::pca(Harman.5)

# Extract eigenvalues
eigenvalues &amp;lt;- res$values

# Graph
plotuScree(x = eigenvalues)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://edoardocostantini.github.io/post/pca-non-graphical-solutions/index_files/figure-html/data-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Non-graphical solutions
ngs &amp;lt;- nScree(x = eigenvalues)

# Kaiser rule\
nkaiser_man &amp;lt;- sum(eigenvalues &amp;gt; 1)

# Accelration factor
a &amp;lt;- NULL
for (j in 2:(length(eigenvalues) - 1)){
  a[j] &amp;lt;- (eigenvalues[j + 1] - eigenvalues[j]) - (eigenvalues[j] - eigenvalues[j - 1])
}

naf_man &amp;lt;- which.max(a) - 1

# Compare results
data.frame(manual = c(naf = naf_man, nkaiser = nkaiser_man),
           nFactor = c(naf = ngs$Components[[&amp;quot;naf&amp;quot;]],
                       nkaiser = ngs$Components[[&amp;quot;nkaiser&amp;quot;]]))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         manual nFactor
## naf          2       2
## nkaiser      2       2&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;tldr-just-give-me-the-code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;TL;DR, just give me the code!&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Prepare environment ----------------------------------------------------------

library(nFactors)
library(psych)

# Perform PCA
res &amp;lt;- psych::pca(Harman.5)

# Extract eigenvalues
eigenvalues &amp;lt;- res$values

# Graph
plotuScree(x = eigenvalues)

# Non-graphical solutions
ngs &amp;lt;- nScree(x = eigenvalues)

# Kaiser rule\
nkaiser_man &amp;lt;- sum(eigenvalues &amp;gt; 1)

# Accelration factor
a &amp;lt;- NULL
for (j in 2:(length(eigenvalues) - 1)){
  a[j] &amp;lt;- (eigenvalues[j + 1] - eigenvalues[j]) - (eigenvalues[j] - eigenvalues[j - 1])
}

naf_man &amp;lt;- which.max(a) - 1

# Compare results
data.frame(manual = c(naf = naf_man, nkaiser = nkaiser_man),
           nFactor = c(naf = ngs$Components[[&amp;quot;naf&amp;quot;]],
                       nkaiser = ngs$Components[[&amp;quot;nkaiser&amp;quot;]]))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;other-resources&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Other resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://rpubs.com/juanhklopper/cross_entropy&#34;&gt;Cross-entropy in RPubs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://machinelearningmastery.com/cross-entropy-for-machine-learning/&#34;&gt;A Gentle Introduction to Cross-Entropy for Machine Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://gombru.github.io/2018/05/23/cross_entropy_loss/&#34;&gt;Understanding Categorical Cross-Entropy Loss, Binary Cross-Entropy Loss, Softmax Loss, Logistic Loss, Focal Loss and all those confusing names&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html&#34;&gt;ML Gloassary&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/swlh/cross-entropy-loss-in-pytorch-c010faf97bab&#34;&gt;Loss Functions in Machine Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
