<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>prediction | Edo</title>
    <link>https://edoardocostantini.github.io/tag/prediction/</link>
      <atom:link href="https://edoardocostantini.github.io/tag/prediction/index.xml" rel="self" type="application/rss+xml" />
    <description>prediction</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Fri, 22 Apr 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://edoardocostantini.github.io/media/icon_hud9fa8ac6cd4a8ee42e0d32197c4c8f6f_199053_512x512_fill_lanczos_center_3.png</url>
      <title>prediction</title>
      <link>https://edoardocostantini.github.io/tag/prediction/</link>
    </image>
    
    <item>
      <title>Cross-entropy as a measure of predictive accuracy</title>
      <link>https://edoardocostantini.github.io/post/cross-entropy/</link>
      <pubDate>Fri, 22 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://edoardocostantini.github.io/post/cross-entropy/</guid>
      <description>
&lt;script src=&#34;https://edoardocostantini.github.io/post/cross-entropy/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1&lt;/span&gt; Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#learn-by-coding&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2&lt;/span&gt; Learn by coding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tldr-just-give-me-the-code&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3&lt;/span&gt; TL;DR, just give me the code!&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#other-resources&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4&lt;/span&gt; Other resources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34; number=&#34;1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Introduction&lt;/h1&gt;
&lt;p&gt;Cross-entropy quantifies the difference between two probability distributions.
As such, it comes in handy as a &lt;a href=&#34;https://en.wikipedia.org/wiki/Loss_function&#34;&gt;loss function&lt;/a&gt; in multi-class classification tasks (e.g., multinomial logistic regression).
Cross-entropy also provides an elegant solution for determining the difference between actual and predicted categorical data point values.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;learn-by-coding&#34; class=&#34;section level1&#34; number=&#34;2&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Learn by coding&lt;/h1&gt;
&lt;p&gt;Bla bla&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Prepare environment ----------------------------------------------------------

# Packages
library(nnet)

# Data
dat_bin &amp;lt;- ToothGrowth
dat_cat &amp;lt;- iris&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First, binary cross entropy&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Binary cross entropy ---------------------------------------------------------

# Fit model
glm_log &amp;lt;- glm(supp ~ .,
               family = binomial(link = &amp;#39;logit&amp;#39;),
               data = ToothGrowth)

# Predictions
preds_log &amp;lt;- predict(glm_log, type = &amp;quot;response&amp;quot;)

# Compute entropy
p &amp;lt;- as.numeric(ToothGrowth$supp)-1 #
phat &amp;lt;- preds_log

x &amp;lt;- 0
for (i in 1:length(p)){
  x &amp;lt;- x + (p[i] * log(phat[i]))
}

- x&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        1 
## 17.99278&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Multi-categorical cross entropy ----------------------------------------------

# Fit model
glm_mln &amp;lt;- multinom(Species ~ ., data = iris)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # weights:  18 (10 variable)
## initial  value 164.791843 
## iter  10 value 16.177348
## iter  20 value 7.111438
## iter  30 value 6.182999
## iter  40 value 5.984028
## iter  50 value 5.961278
## iter  60 value 5.954900
## iter  70 value 5.951851
## iter  80 value 5.950343
## iter  90 value 5.949904
## iter 100 value 5.949867
## final  value 5.949867 
## stopped after 100 iterations&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Predictions
preds_mln &amp;lt;- predict(glm_mln, type = &amp;quot;probs&amp;quot;)

# Compute entropy
p &amp;lt;- FactoMineR::tab.disjonctif(iris$Species)
phat &amp;lt;- preds_mln

x &amp;lt;- 0
for (i in 1:nrow(p)){
  for (j in 1:ncol(p)){
    x &amp;lt;- x + (p[i, ] * log(phat[i, ]))
  }
}

- sum(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 17.8496&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Fast way to compute both -----------------------------------------------------

ce &amp;lt;- - sum(
  diag(
    p %*% t(log(phat))
  )
)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;tldr-just-give-me-the-code&#34; class=&#34;section level1&#34; number=&#34;3&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; TL;DR, just give me the code!&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Prepare environment ----------------------------------------------------------

# Packages
library(nnet)

# Data
dat_bin &amp;lt;- ToothGrowth
dat_cat &amp;lt;- iris

# Binary cross entropy ---------------------------------------------------------

# Fit model
glm_log &amp;lt;- glm(supp ~ .,
               family = binomial(link = &amp;#39;logit&amp;#39;),
               data = ToothGrowth)

# Predictions
preds_log &amp;lt;- predict(glm_log, type = &amp;quot;response&amp;quot;)

# Compute entropy
p &amp;lt;- as.numeric(ToothGrowth$supp)-1 #
phat &amp;lt;- preds_log

x &amp;lt;- 0
for (i in 1:length(p)){
  x &amp;lt;- x + (p[i] * log(phat[i]))
}

- x

# Multi-categorical cross entropy ----------------------------------------------

# Fit model
glm_mln &amp;lt;- multinom(Species ~ ., data = iris)

# Predictions
preds_mln &amp;lt;- predict(glm_mln, type = &amp;quot;probs&amp;quot;)

# Compute entropy
p &amp;lt;- FactoMineR::tab.disjonctif(iris$Species)
phat &amp;lt;- preds_mln

x &amp;lt;- 0
for (i in 1:nrow(p)){
  for (j in 1:ncol(p)){
    x &amp;lt;- x + (p[i, ] * log(phat[i, ]))
  }
}

- sum(x)

# Fast way to compute both -----------------------------------------------------

ce &amp;lt;- - sum(
  diag(
    p %*% t(log(phat))
  )
)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;other-resources&#34; class=&#34;section level1&#34; number=&#34;4&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Other resources&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://rpubs.com/juanhklopper/cross_entropy&#34;&gt;Cross-entropy in RPubs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://machinelearningmastery.com/cross-entropy-for-machine-learning/&#34;&gt;A Gentle Introduction to Cross-Entropy for Machine Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://gombru.github.io/2018/05/23/cross_entropy_loss/&#34;&gt;Understanding Categorical Cross-Entropy Loss, Binary Cross-Entropy Loss, Softmax Loss, Logistic Loss, Focal Loss and all those confusing names&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
